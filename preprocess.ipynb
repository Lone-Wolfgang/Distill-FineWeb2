{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group FineWeb Data by Domain  \n",
    "\n",
    "Working with FineWeb data is challenging due to the size of the corpus. For example, the Japanese segment of FineWeb consists of 370 million examples, split into 148 files and totaling about 500 GB of data. I am using a laptop with 16 GB of RAM, and I imagine many of you are operating under even stricter constraints. To address this, I am developing a library specifically designed to process FineWeb data using strategies that prioritize memory efficiency.  \n",
    "\n",
    "The leadership of FineWeb has requested that the community select a sample of data that is potentially high-quality educational material.  \n",
    "\n",
    "To facilitate my search of the Japanese segment, I have grouped the raw data by domain and consolidated it into a single DataFrame. This approach makes it easy to identify the websites that comprise the FineWeb corpus and how many pages each contributed.  \n",
    "\n",
    "## Setup  \n",
    "\n",
    "Before we get started, please install **fineweb_tools**, which will automatically install all dependencies required to run this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fineweb-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python library, **fineweb_tools**, is purpose-built for FineWeb data. It prioritizes minimal dependencies and memory efficiency.  \n",
    "\n",
    "DataFrame operations are performed with **polars**, which is installed as a dependency of **fineweb_tools**.  \n",
    "\n",
    "Let's start by looking at the final output of preprocessing:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Domains: 7486452\n",
      "Total Pages 376134745\n",
      "\n",
      "shape: (10, 3)\n",
      "┌─────────────────────────────────┬─────────┬───────┐\n",
      "│ domain                          ┆ count   ┆ tld   │\n",
      "│ ---                             ┆ ---     ┆ ---   │\n",
      "│ str                             ┆ u32     ┆ str   │\n",
      "╞═════════════════════════════════╪═════════╪═══════╡\n",
      "│ http://lineq.jp                 ┆ 1346922 ┆ jp    │\n",
      "│ https://ameblo.jp               ┆ 908818  ┆ jp    │\n",
      "│ http://ameblo.jp                ┆ 773217  ┆ jp    │\n",
      "│ https://oshiete.goo.ne.jp       ┆ 770550  ┆ ne.jp │\n",
      "│ https://www.amazon.co.jp        ┆ 695980  ┆ co.jp │\n",
      "│ http://mixi.jp                  ┆ 471151  ┆ jp    │\n",
      "│ http://news.livedoor.com        ┆ 452297  ┆ com   │\n",
      "│ https://detail.chiebukuro.yaho… ┆ 408322  ┆ co.jp │\n",
      "│ http://q.hatena.ne.jp           ┆ 386786  ┆ ne.jp │\n",
      "│ https://qa.mamari.jp            ┆ 337377  ┆ jp    │\n",
      "└─────────────────────────────────┴─────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_parquet('preprocessed/jpn_Jpan.parquet')\n",
    "print('Total Domains:', df.height)\n",
    "print('Total Pages', df['count'].sum())\n",
    "print()\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 7 million unique domains, but a significant portion of the data comes from a small number of prolific contributors. I am reviewing the largest contributors individually to assess their educational value.  \n",
    "\n",
    "Most of these domains lack educational merit, and excluding them from future samples could improve overall quality.  \n",
    "\n",
    "However, some websites contain useful material. When I find a promising site, I focus on identifying high-quality pages, scraping their links, and matching them to the FineWeb raw data to extract the corresponding IDs.  \n",
    "\n",
    "This notebook is a guide for using **fineweb_tools** for preprocessing. Web scraping is covered in another notebook.  \n",
    "\n",
    "## Structure  \n",
    "\n",
    "1. **Quickstart**: Set up your environment, run a sanity check, and preprocess your language data.  \n",
    "2. **Pipeline**: Explains each step of the pipeline in more detail.  \n",
    "3. **Conclusion**: Briefly analyzes the Japanese FineWeb corpus contents and discusses future directions.  \n",
    "\n",
    "## Quickstart  \n",
    "\n",
    "This section will guide you in preprocessing your data without getting too deep into the details.  \n",
    "\n",
    "1. **Find Your Language**: Query the HF hub to determine the code assigned to your target language.  \n",
    "2. **Run a Sanity Check**: Run the preprocessing pipeline on a sample to ensure everything is functioning correctly.  \n",
    "3. **Preprocess**: Run the pipeline on your full language dataset.  \n",
    "\n",
    "### Find Your Language  \n",
    "\n",
    "The first step is to find your language code. There are over a thousand different languages in the repository. The function **get_fineweb_languages** queries the repository for the languages it contains.  \n",
    "\n",
    "It's not always obvious how the language is named. Use a **list comprehension** to search for your language.  \n",
    "\n",
    "Alternatively, you can visit the [FineWeb2 HF Hub](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) and find your language code in the dropdown menu.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Languages: 1893\n",
      "Sample: ['tso_Latn', 'kas_Arab', 'sby_Latn', 'avk_Latn', 'doi_Deva']\n",
      "\n",
      "Languages with \"jp\": ['jpn_Jpan', 'bjp_Latn', 'cjp_Latn', 'ljp_Latn']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from modules.preprocess import get_fineweb_languages\n",
    "\n",
    "languages = get_fineweb_languages()\n",
    "print ('Total Languages:', len(languages))\n",
    "print ('Sample:', random.sample(languages, 5))\n",
    "print()\n",
    "###Use a list comprehension to find your language.\n",
    "print('Languages with \"jp\":', [lang for lang in languages if 'jp' in lang])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Sanity Check  \n",
    "\n",
    "Preprocessing is a time-consuming operation, and it's best to ensure everything is in order before you preprocess the entire language dataset.  \n",
    "\n",
    "Run the pipeline with the **sample** argument, and it will operate on the specified number of files. The preprocessing functions are configured resume progress from previously completed operations. So, there is no time lost in running a sanity check.\n",
    "\n",
    "Upon completion, the raw data, intermediates, and outputs will be saved to the following directory structure:  \n",
    "\n",
    "\n",
    "    root/\n",
    "    ├── FineWeb/data/{language}/train\n",
    "    │                            ├── 000_00000.parquet\n",
    "    │                            ├── 000_00001.parquet\n",
    "    │                            └── 000_00002.parquet\n",
    "    ├── intermediate/\n",
    "    │   ├── stripped/{language}\n",
    "    │   │               ├── 000_00000.parquet\n",
    "    │   │               ├── 000_00001.parquet\n",
    "    │   │               └── 000_00002.parquet\n",
    "    │   ├── grouped/{language}\n",
    "    │   │               ├── 000_00000.parquet\n",
    "    │   │               ├── 000_00001.parquet\n",
    "    │   │               └── 000_00002.parquet\n",
    "    ├── preprocessed/\n",
    "            └── {language}.parquet\n",
    "\n",
    "Let's run a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking FineWeb hub for the language code, \"bjp_Latn\"...\n",
      "Running sanity check. 1 paths sampled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading FineWeb data:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5e6e9c652743518105a5f219ed7920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "000_00000.parquet:   0%|          | 0.00/222k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading FineWeb data: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete. Saved raw FineWeb data to FineWeb\\data\\bjp_Latn\\train\\data\\bjp_Latn\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stripping dfs and adding domain column: 100%|██████████| 1/1 [00:00<00:00, 925.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. 1 processed. Saved files to intermediate\\stripped\\bjp_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping by domain: 100%|██████████| 1/1 [00:00<00:00, 321.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. 1 processed. Saved output to intermediate\\grouped\\bjp_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining grouped data: 100%|██████████| 1/1 [00:00<00:00, 75.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataFrames combined. Adding top-level domain column.\n",
      "Saved combined dataframe to preprocessed\\bjp_Latn.parquet\n",
      "shape: (10, 3)\n",
      "┌─────────────────────────────────┬───────┬──────────────┐\n",
      "│ domain                          ┆ count ┆ tld          │\n",
      "│ ---                             ┆ ---   ┆ ---          │\n",
      "│ str                             ┆ u32   ┆ str          │\n",
      "╞═════════════════════════════════╪═══════╪══════════════╡\n",
      "│ https://png.bible               ┆ 83    ┆ bible        │\n",
      "│ https://www.stepbible.org       ┆ 24    ┆ org          │\n",
      "│ http://akustressgiler.blogspot… ┆ 19    ┆ blogspot.com │\n",
      "│ https://ebible.org              ┆ 12    ┆ org          │\n",
      "│ https://www.anagrammen.com      ┆ 2     ┆ com          │\n",
      "│ http://fatlike.worddetector.co… ┆ 1     ┆ com          │\n",
      "│ https://en.wikisource.org       ┆ 1     ┆ org          │\n",
      "│ http://www.anagrammen.com       ┆ 1     ┆ com          │\n",
      "│ http://zlobek.poznan.pl         ┆ 1     ┆ poznan.pl    │\n",
      "│ http://glaikit.worddetector.co… ┆ 1     ┆ com          │\n",
      "└─────────────────────────────────┴───────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "from fineweb_tools.preprocess import download_and_preprocess_pipeline\n",
    "\n",
    "#I run the code on 'bjp_Latn', which is a much smaller dataset than Japanese.\n",
    "download_and_preprocess_pipeline(\n",
    "    language = 'bjp_Latn',\n",
    "    sample = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess  \n",
    "\n",
    "Now, we will run the code on the full Japanese dataset.  \n",
    "\n",
    "The Japanese dataset is too large for my internal hard drive, so I save my language files to an external hard drive using the **local_dir** argument.  \n",
    "\n",
    "The final step of the pipeline is the most memory-intensive. DataFrames are combined in batches, controlled by the **batch_size** argument. The default value is set low (10) to avoid memory issues. You can increase the batch size to speed up the process, depending on your system's capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking FineWeb hub for the language code, \"jpn_Jpan\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading FineWeb data: 100%|██████████| 148/148 [00:00<00:00, 12487.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete. Saved raw FineWeb data to D:\\FineWeb\\data\\jpn_Jpan\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stripping dfs and adding domain column: 100%|██████████| 148/148 [00:00<00:00, 14513.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. 148 processed. Saved files to intermediate\\stripped\\jpn_Jpan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping by domain: 100%|██████████| 148/148 [00:00<00:00, 14633.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. 148 processed. Saved output to intermediate\\grouped\\jpn_Jpan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining grouped data: 100%|██████████| 5/5 [00:44<00:00,  8.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataFrames combined. Adding top-level domain column.\n",
      "Saved combined dataframe to preprocessed\\jpn_Jpan.parquet\n",
      "shape: (10, 3)\n",
      "┌─────────────────────────────────┬─────────┬───────┐\n",
      "│ domain                          ┆ count   ┆ tld   │\n",
      "│ ---                             ┆ ---     ┆ ---   │\n",
      "│ str                             ┆ u32     ┆ str   │\n",
      "╞═════════════════════════════════╪═════════╪═══════╡\n",
      "│ http://lineq.jp                 ┆ 1346922 ┆ jp    │\n",
      "│ https://ameblo.jp               ┆ 908818  ┆ jp    │\n",
      "│ http://ameblo.jp                ┆ 773217  ┆ jp    │\n",
      "│ https://oshiete.goo.ne.jp       ┆ 770550  ┆ ne.jp │\n",
      "│ https://www.amazon.co.jp        ┆ 695980  ┆ co.jp │\n",
      "│ http://mixi.jp                  ┆ 471151  ┆ jp    │\n",
      "│ http://news.livedoor.com        ┆ 452297  ┆ com   │\n",
      "│ https://detail.chiebukuro.yaho… ┆ 408322  ┆ co.jp │\n",
      "│ http://q.hatena.ne.jp           ┆ 386786  ┆ ne.jp │\n",
      "│ https://qa.mamari.jp            ┆ 337377  ┆ jp    │\n",
      "└─────────────────────────────────┴─────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "download_and_preprocess_pipeline(\n",
    "    local_dir='D:/FineWeb',\n",
    "    language = 'jpn_Jpan',\n",
    "    batch_size = 30 ## Works well with 16GB of RAM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished! Your language data is preproccessed.\n",
    "\n",
    "The next section explains the pipeline in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipepline\n",
    "\n",
    "Preprocessing involves four steps:\n",
    "\n",
    "1. **Download**: Collect language data from HF Hub.\n",
    "2. **Strip**: Unnecessary columns are removed, and the domain column is added.\n",
    "3. **Group**: DataFrames are grouped by domain.\n",
    "4. **Combine**: Combines grouped data into a single DataFrame and adds the Top Level Domain in the **tld** column.\n",
    "\n",
    "For this tutorial, I'm going to use a very small language group, **bjp_Latn**.\n",
    "\n",
    "### Download Data from HF Hub\n",
    "\n",
    "To download data from FineWeb, all you need is the code for your target language. Some of the language sets are very large, so I set the function up to raise an error unless the input language code exactly matches one from FineWeb.  \n",
    "\n",
    "To learn how to find your language code, please check the Quickstart section at the beginning.  \n",
    "\n",
    "This is a very straightforward function. It takes the following arguments:  \n",
    "- **language (str)**: The code for your target language.  \n",
    "- **local_dir (str)**: Where to save your FineWeb data. Defaults to 'FineWeb'.  \n",
    "- **sample (int)**: Optional. If provided, will sample N paths. Useful for a sanity check.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking FineWeb hub for the language code, \"bjp_Latn\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading FineWeb data:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465b1c88d9c642e28db63133d5014ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "000_00000.parquet:   0%|          | 0.00/222k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading FineWeb data: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete. Saved raw FineWeb data to FineWeb\\data\\bjp_Latn\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.preprocess import download_fineweb_data\n",
    "\n",
    "download_fineweb_data(\n",
    "    language = 'bjp_Latn',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default settings, your directory will look like this:\n",
    "\n",
    "    root/\n",
    "    ├── FineWeb/data/{language}/train\n",
    "                                 └── 000_00000.parquet\n",
    "\n",
    "### Strip\n",
    "\n",
    "The first step in preprocessing is to remove unneccesary data in order to lighten the memory load.\n",
    "\n",
    "Let's start by looking at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': String,\n",
       " 'id': String,\n",
       " 'dump': String,\n",
       " 'url': String,\n",
       " 'date': String,\n",
       " 'file_path': String,\n",
       " 'language': String,\n",
       " 'language_score': Float64,\n",
       " 'language_script': String,\n",
       " 'minhash_cluster_size': Int64,\n",
       " 'top_langs': String}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "#Use the polars function, read_parquet_schema, to quickly load columns and datatypes\n",
    "pl.read_parquet_schema('FineWeb/data/bjp_Latn/train/000_00000.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of columns, and most of them are not useful for our purposes.\n",
    "\n",
    "- **text**: The main data. The text contents of the webpage.\n",
    "- **id**: Identifies the row in the FineWeb corpus.\n",
    "- **dump**: Group of files from Common Crawl that the text was extracted from.\n",
    "- **url**: URL from which the text was extracted.\n",
    "- **date**: The date the crawl was performed.\n",
    "- **file_path**: Exact file from which the text was extracted.\n",
    "- **language**: Most likely language.\n",
    "- **language_score**: Confidence that the identified language is the true language.\n",
    "- **language_script**: Character family used to write this language.\n",
    "- **min_hash_cluster_size**: How many other text samples were marked as near-duplicates.\n",
    "- **top_langs**: Rank-ordered dictionary of languages and their confidence scores.\n",
    "\n",
    "All we need to group by domain is the **url** column.\n",
    "\n",
    "Additionally, I set up the default settings to keep the **date** and **id**. This will lighten the load enough so I can store these files on my internal hard drive.  \n",
    "\n",
    "Now, I have a lightweight option to query IDs and analyze the time distribution.\n",
    "\n",
    "Next, let's strip the data and add the domain columns.\n",
    "\n",
    "The default settings keep the columns `['url', 'id', 'date']`. You can select the columns you want. However, if you don't select the **url** column, or if you select a column that does not exist, it will raise an error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stripping dfs and adding domain column: 100%|██████████| 1/1 [00:00<00:00, 996.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. 1 processed. Saved files to intermediate\\stripped\\bjp_Latn\n",
      "shape: (5, 4)\n",
      "┌─────────────────────────┬──────────────────────┬────────────────────────┬────────────────────────┐\n",
      "│ id                      ┆ date                 ┆ url                    ┆ domain                 │\n",
      "│ ---                     ┆ ---                  ┆ ---                    ┆ ---                    │\n",
      "│ str                     ┆ str                  ┆ str                    ┆ str                    │\n",
      "╞═════════════════════════╪══════════════════════╪════════════════════════╪════════════════════════╡\n",
      "│ <urn:uuid:e7538e04-663b ┆ 2014-07-24T02:34:00Z ┆ http://akustressgiler. ┆ http://akustressgiler. │\n",
      "│ -476b-8…                ┆                      ┆ blogspot…              ┆ blogspot…              │\n",
      "│ <urn:uuid:6ffe8cb5-13a2 ┆ 2014-07-24T22:59:29Z ┆ http://akustressgiler. ┆ http://akustressgiler. │\n",
      "│ -4497-a…                ┆                      ┆ blogspot…              ┆ blogspot…              │\n",
      "│ <urn:uuid:9cdce1c3-cc1d ┆ 2015-01-29T04:19:04Z ┆ http://akustressgiler. ┆ http://akustressgiler. │\n",
      "│ -4aa0-b…                ┆                      ┆ blogspot…              ┆ blogspot…              │\n",
      "│ <urn:uuid:6cf8fffa-b77a ┆ 2015-01-29T18:22:50Z ┆ http://akustressgiler. ┆ http://akustressgiler. │\n",
      "│ -4a70-a…                ┆                      ┆ blogspot…              ┆ blogspot…              │\n",
      "│ <urn:uuid:eb7f8346-b07d ┆ 2015-01-30T13:58:13Z ┆ http://akustressgiler. ┆ http://akustressgiler. │\n",
      "│ -4173-8…                ┆                      ┆ blogspot…              ┆ blogspot…              │\n",
      "└─────────────────────────┴──────────────────────┴────────────────────────┴────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.preprocess import strip_and_add_domain_pipeline\n",
    "\n",
    "strip_and_add_domain_pipeline(\n",
    "    local_dir=\"FineWeb\",\n",
    "    language = 'bjp_Latn'\n",
    ")\n",
    "\n",
    "print(pl.read_parquet('intermediate/stripped/bjp_Latn/000_00000.parquet').head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your directory should look like this:\n",
    "\n",
    "    root/\n",
    "    ├── FineWeb/data/{language}/train\n",
    "    │                           └── 000_00000.parquet\n",
    "    └── intermediate/\n",
    "        └── stripped/{language}\n",
    "                       └── 000_00000.parquet\n",
    "\n",
    "Next, let's group the data.\n",
    "\n",
    "### Group\n",
    "\n",
    "Iterates through dataframes and groups them by domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping by domain: 100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. 1 processed. Saved output to intermediate\\grouped\\bjp_Latn\n",
      "shape: (20, 2)\n",
      "┌─────────────────────────────────┬───────┐\n",
      "│ domain                          ┆ count │\n",
      "│ ---                             ┆ ---   │\n",
      "│ str                             ┆ u32   │\n",
      "╞═════════════════════════════════╪═══════╡\n",
      "│ https://www.bible.com           ┆ 1     │\n",
      "│ https://loginseekgo.com         ┆ 1     │\n",
      "│ http://fatlike.worddetector.co… ┆ 1     │\n",
      "│ http://zlobek.poznan.pl         ┆ 1     │\n",
      "│ http://global-cheap-hotels.com  ┆ 1     │\n",
      "│ …                               ┆ …     │\n",
      "│ https://ebible.org              ┆ 12    │\n",
      "│ http://akustressgiler.blogspot… ┆ 19    │\n",
      "│ https://png.bible               ┆ 83    │\n",
      "│ http://talkie.worddetector.com  ┆ 1     │\n",
      "│ http://www.anagrammen.com       ┆ 1     │\n",
      "└─────────────────────────────────┴───────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.preprocess import group_pipeline\n",
    "\n",
    "group_pipeline(\n",
    "    language = 'bjp_Latn'\n",
    ")\n",
    "\n",
    "print (pl.read_parquet('intermediate/grouped/bjp_Latn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function groups stripped files into dataframes with 2 columns:\n",
    "- **domain**: The domain from which URLs were extracted.\n",
    "- **count**: The number of pages which that URL contributed.\n",
    "\n",
    "Now, your directory will look like this:\n",
    "\n",
    "    root/\n",
    "    ├── FineWeb/data/{language}/train\n",
    "    │                           └── 000_00000.parquet\n",
    "    └── intermediate/\n",
    "        ├── stripped/{language}\n",
    "        │               └── 000_00000.parquet\n",
    "        └── grouped/{language}\n",
    "                        └── 000_00000.parquet\n",
    "### Combine\n",
    "\n",
    "Finally, let's combine our DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining grouped data: 100%|██████████| 1/1 [00:00<00:00, 998.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataFrames combined. Adding top-level domain column.\n",
      "Saved combined dataframe to preprocessed\\bjp_Latn.parquet\n",
      "shape: (10, 3)\n",
      "┌─────────────────────────────────┬───────┬──────────────┐\n",
      "│ domain                          ┆ count ┆ tld          │\n",
      "│ ---                             ┆ ---   ┆ ---          │\n",
      "│ str                             ┆ u32   ┆ str          │\n",
      "╞═════════════════════════════════╪═══════╪══════════════╡\n",
      "│ https://png.bible               ┆ 83    ┆ bible        │\n",
      "│ https://www.stepbible.org       ┆ 24    ┆ org          │\n",
      "│ http://akustressgiler.blogspot… ┆ 19    ┆ blogspot.com │\n",
      "│ https://ebible.org              ┆ 12    ┆ org          │\n",
      "│ https://www.anagrammen.com      ┆ 2     ┆ com          │\n",
      "│ http://tealike.worddetector.co… ┆ 1     ┆ com          │\n",
      "│ http://zlobek.poznan.pl         ┆ 1     ┆ poznan.pl    │\n",
      "│ http://faiking.worddetector.co… ┆ 1     ┆ com          │\n",
      "│ http://fatlike.worddetector.co… ┆ 1     ┆ com          │\n",
      "│ https://www.bible.com           ┆ 1     ┆ com          │\n",
      "└─────────────────────────────────┴───────┴──────────────┘\n",
      "<function read_parquet at 0x000001A9B61B7BE0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.preprocess import combine_grouped_dfs\n",
    "\n",
    "combine_grouped_dfs(\n",
    "    language = 'bjp_Latn'\n",
    ")\n",
    "\n",
    "print(pl.read_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test case, there is only one language file, so the output is similar to the previous one. This function also sorts and adds the **Top-Level Domain** column, **tld**.\n",
    "\n",
    "TLD is particularly useful in cases where language confidence is low. Please refer to this [blog post](https://danielvanstrien.xyz/posts/2024/12/23/fineweb-filter-polars.html).\n",
    "\n",
    "This is the final step in preprocessing, and your directory should look like this:\n",
    "\n",
    "    root/\n",
    "    ├── FineWeb/data/{language}/train\n",
    "    │                           └── 000_00000.parquet\n",
    "    └── intermediate/\n",
    "        ├── stripped/{language}\n",
    "        │               └── 000_00000.parquet\n",
    "        ├── grouped/{language}\n",
    "        │               └── 000_00000.parquet\n",
    "        └── preprocessed\n",
    "                └──{language}.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "For this tutorial, we will conclude with an overview of the Japanese segment of FineWeb, grouped by domains and the number of pages which they contributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Domains: 7,486,452\n",
      "Total URLs: 376,134,745\n",
      "shape: (7, 6)\n",
      "┌───────┬───────────┬───────────┬─────────┬───────────┬─────────────┐\n",
      "│ group ┆ group_min ┆ group_max ┆ domains ┆ pages     ┆ corpus_perc │\n",
      "│ ---   ┆ ---       ┆ ---       ┆ ---     ┆ ---       ┆ ---         │\n",
      "│ i32   ┆ u32       ┆ u32       ┆ u32     ┆ u32       ┆ f64         │\n",
      "╞═══════╪═══════════╪═══════════╪═════════╪═══════════╪═════════════╡\n",
      "│ 0     ┆ 1         ┆ 9         ┆ 4917634 ┆ 13986881  ┆ 3.72        │\n",
      "│ 1     ┆ 10        ┆ 99        ┆ 1984799 ┆ 63838337  ┆ 16.97       │\n",
      "│ 2     ┆ 100       ┆ 999       ┆ 541777  ┆ 145888934 ┆ 38.79       │\n",
      "│ 3     ┆ 1000      ┆ 9996      ┆ 40324   ┆ 85025677  ┆ 22.61       │\n",
      "│ 4     ┆ 10001     ┆ 99243     ┆ 1812    ┆ 44257590  ┆ 11.77       │\n",
      "│ 5     ┆ 100078    ┆ 908818    ┆ 105     ┆ 21790404  ┆ 5.79        │\n",
      "│ 6     ┆ 1346922   ┆ 1346922   ┆ 1       ┆ 1346922   ┆ 0.36        │\n",
      "└───────┴───────────┴───────────┴─────────┴───────────┴─────────────┘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-b24268bb3a7d4694af4841debb4a5ac2.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-b24268bb3a7d4694af4841debb4a5ac2.vega-embed details,\n",
       "  #altair-viz-b24268bb3a7d4694af4841debb4a5ac2.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-b24268bb3a7d4694af4841debb4a5ac2\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-b24268bb3a7d4694af4841debb4a5ac2\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-b24268bb3a7d4694af4841debb4a5ac2\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-b4fb3901ff329941a620a1c09ffe4abc\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"group\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pages\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_3\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-b4fb3901ff329941a620a1c09ffe4abc\": [{\"group\": 0, \"group_min\": 1, \"group_max\": 9, \"domains\": 4917634, \"pages\": 13986881, \"corpus_perc\": 3.72}, {\"group\": 1, \"group_min\": 10, \"group_max\": 99, \"domains\": 1984799, \"pages\": 63838337, \"corpus_perc\": 16.97}, {\"group\": 2, \"group_min\": 100, \"group_max\": 999, \"domains\": 541777, \"pages\": 145888934, \"corpus_perc\": 38.79}, {\"group\": 3, \"group_min\": 1000, \"group_max\": 9996, \"domains\": 40324, \"pages\": 85025677, \"corpus_perc\": 22.61}, {\"group\": 4, \"group_min\": 10001, \"group_max\": 99243, \"domains\": 1812, \"pages\": 44257590, \"corpus_perc\": 11.77}, {\"group\": 5, \"group_min\": 100078, \"group_max\": 908818, \"domains\": 105, \"pages\": 21790404, \"corpus_perc\": 5.79}, {\"group\": 6, \"group_min\": 1346922, \"group_max\": 1346922, \"domains\": 1, \"pages\": 1346922, \"corpus_perc\": 0.36}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules.analyze import group_domains_by_count\n",
    "\n",
    "df = pl.read_parquet('preprocessed/jpn_Jpan.parquet')\n",
    "result = group_domains_by_count(df)\n",
    "print(result)\n",
    "result.plot.bar(\n",
    "    x='group',\n",
    "    y='pages'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we group the domains like this, we observe that a significant portion of the corpus is contributed by very large websites. For example, just 100 websites contributed 21 million pages, and approximately 2,000 websites contributed 45 million pages, comprising 16% of the corpus.\n",
    "\n",
    "I plan to begin by examining these large websites. My expectation is that most of them will hold little to no value. However, if we can identify and exclude these low-value pages in future random samples, it could improve the overall quality of the corpus. Conversely, if a few of these domains prove to be useful, targeting them could be a quick and effective way to enhance the representation of educational material.\n",
    "\n",
    "In addition, I plan to ask my Japanese friends to recommend high-quality educational websites. I don't have a specific approach in mind—I will simply ask them to share educational materials they personally use or would recommend to others.\n",
    "\n",
    "I will classify these websites using the system we have already established:\n",
    "\n",
    "1. **No Educational Value**: Light news, e-commerce, sports, personal blogs, business pages, etc.\n",
    "2. **Minimal Educational Value**: Mostly amateur content. With a lot of pruning, potentially a good resource for Minimal or Basic educational content. Ex. QA forums, SEO Content Disguised as Educational Blogs.\n",
    "Examples: Quora, [Oshiete](https://oshiete.goo.ne.jp/watch/pro/), [RareJob](https://www.rarejob.com/englishlab/)\n",
    "3. **Basic Educational Value**: Mostly amateur content, but overall, education is the priority. Potentially a good resource for Minimal, Basic, or Good educational content.\n",
    "Examples: StackOverflow, [Qiita](https://qiita.com/)\n",
    "4. **Good Educational Value**: Education is clearly the priority, but their may be some issues. Maybe it covers a lot of topics, but it doesn't go into any topic in too much depth, such as WikiPedia. Maybe there is a lot of high-quality content, but also a large amount of non-educational content, such as HuggingFace.\n",
    "5. **Excellent Educational Value**: The entire website is dedicated to providing education on a certain topic, and it explores that topic in great depth. Randomly sample any page on this website, and you will likely draw one that is Good or Excellent educational value.\n",
    "Examples: [NLTK Book](https://www.nltk.org/book/), [Imabi](https://imabi.org/)\n",
    "\n",
    "If this strategy of targeting specific websites and querying the existing FineWeb repository proves effective, I propose soliciting the community for additional high-quality website contributions in their native languages.\n",
    "\n",
    "Thank you for reading. I will continue to update this notebook, and I hope it was useful for you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
