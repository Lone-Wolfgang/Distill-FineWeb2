{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group FineWeb Data by Domain to Target URLs\n",
    "\n",
    "Hello, everyone. I'm working on the Japanese portion of FineWeb. During annotation, I noticed that the density of educational content is quite low. So far, fewer than 1% of examples have scored 3 or higher. To scale up the educational content, we’ll need a significantly larger sample.\n",
    "\n",
    "Educational content is sparse. Most web material is geared toward entertainment or profit. Furthermore, isolating educational content from high-quality websites is not trivial. For instance, if you were to randomly select a URL from a site like HuggingFace, it’s highly likely that you’d end up with a catalog page.\n",
    "\n",
    "For languages with a low density of educational material, the leadership of FineWeb has requested a list of high-quality URLs and IDs to prepare an additional sample for annotation.\n",
    "\n",
    "My primary challenge is limited compute resources. Currently, I’m living abroad with a constrained setup: a 500 GB internal hard drive, a 2 TB external hard drive, and 16 GB of RAM. The Japanese segment of the dataset is quite large, totaling about 500 GB and divided into 146 shards.\n",
    "\n",
    "In this notebook, I’ll share how I extracted a sample of IDs that are potentially high quality.\n",
    "\n",
    "## Structure\n",
    "\n",
    "1. **Setup**: How to prepare your environment for the task.\n",
    "2. **Preprocessing**: From the raw FineWeb dataset hosted on HuggingFace-Hub, strip away unnecessary data and group by domain for efficient browsing.\n",
    "3. **Analysis**: Inspect the distribution of the corpus by domain and select domains with high-quality content.\n",
    "4. **Scraping**: From the target domains, crawl a portion of the sitemap and compile a list of URLs.\n",
    "5. **ID Extraction**: Using the list of URLs from scraping, compile the corresponding IDs from the FineWeb corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To run this code, you will need to set up an environment a few libraries. After installing, you can run the import statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install polars huggingface_hub tqdm tld --upgrade beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from math import log10\n",
    "import os\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import re\n",
    "import requests\n",
    "from tld import get_tld\n",
    "from tqdm.auto import tqdm\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increase amount of data polars shows\n",
    "pl.Config.set_tbl_rows(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "For my setup, I have the Japanese segment downloaded to my external hard drive. This works well for me, but you might find it easier to load the data directly from the HuggingFace Hub. If you need guidance on how to do this, please refer to the [blog post](https://danielvanstrien.xyz/posts/2024/12/23/fineweb-filter-polars.html).\n",
    "\n",
    "We will be using the `polars` library, which offers powerful tools for working with large files without overloading your RAM. To begin, I want to inspect the columns in the dataset and keep only the ones I need. The function `pl.read_parquet_schema` checks a file and returns a dictionary of columns with datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': String,\n",
       " 'id': String,\n",
       " 'dump': String,\n",
       " 'url': String,\n",
       " 'date': String,\n",
       " 'file_path': String,\n",
       " 'language': String,\n",
       " 'language_score': Float64,\n",
       " 'language_script': String,\n",
       " 'minhash_cluster_size': Int64,\n",
       " 'top_langs': String}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = r\"D:\\FineWeb\\jpn_Jpan\\train\"\n",
    "pl.read_parquet_schema(Path(data_dir, os.listdir(data_dir)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will keep only the columns `'id'`, `'url'`, and `'date'`. Dropping the other columns, especially `'text'`, will significantly reduce the memory load. \n",
    "\n",
    "As we continue preprocessing, we will use the `pl.read_parquet` function. This function behaves similarly to its counterpart in pandas, offering a familiar interface while being optimized for performance.\n",
    "\n",
    "### Step 1: Strip\n",
    "In this next block of code, will will load the data, remove unneccesary columns, and add the `'domain'` column. Data files will be saved for the next step of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 148/148 [00:00<00:00, 4354.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the function for extracting the domain\n",
    "def get_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "\n",
    "# Given a datafrome, this function will add the 'domain' column.\n",
    "def add_domain_column(df):\n",
    "    df = df.with_columns(\n",
    "    pl.col(\"url\").map_elements(lambda x: get_domain(x), return_dtype=pl.Utf8).alias(\"domain\")\n",
    "    )\n",
    "    return df.drop_nans()\n",
    "\n",
    "# The main function iterates through files, adds the domain column, and saves the dataframes to a new directory.\n",
    "def strip_and_add_domain_pipeline(\n",
    "        data_dir = r\"D:\\FineWeb\\jpn_Jpan\\train\",\n",
    "        output_dir = \"preprocess/stripped\",\n",
    "        columns = ['id', 'date', 'url']\n",
    "):\n",
    "    files = os.listdir(data_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for file in tqdm(files, desc= \"Preprocessing\"):\n",
    "        input_path, output_path = Path(data_dir, file), Path(output_dir, file)\n",
    "        if os.path.exists(output_path):\n",
    "            continue\n",
    "        else:\n",
    "                df = add_domain_column(pl.read_parquet(input_path, columns=columns))\n",
    "                df.write_parquet(output_path)\n",
    "        \n",
    "strip_and_add_domain_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Group\n",
    "\n",
    "Next, dataframes will be grouped by the domain. Grouped frames will be saved individually before being compiled in the final step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   0%|          | 0/148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 148/148 [00:00<00:00, 7011.75it/s]\n"
     ]
    }
   ],
   "source": [
    "def group_pipeline(\n",
    "    input_dir = 'preprocess/stripped',\n",
    "    output_dir = 'preprocess/grouped'\n",
    "):\n",
    "    files = os.listdir(input_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for file in tqdm(files, desc= 'Preprocessing'):\n",
    "        input_path, output_path = Path(data_dir, file), Path(output_dir, file)\n",
    "        if os.path.exists(output_path):\n",
    "            continue\n",
    "        else:\n",
    "            df = pl.read_parquet(input_path)\n",
    "            df = df.group_by('domain').count()\n",
    "            df.write_parquet(output_path)\n",
    "\n",
    "group_pipeline()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compile\n",
    "\n",
    "In this final preprocessing step, will combine all the grouped data. I ran this in batches to avoid running out of memory.\n",
    "\n",
    "After combining all the data, I add the column with Top Level Domain (tld), sort it, and saved the preprocessed file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:40<00:00,  8.00s/it]\n"
     ]
    }
   ],
   "source": [
    "def compile(\n",
    "        input_dir='preprocess/grouped',\n",
    "        output_path='preprocessed.parquet',\n",
    "        batch_size=30\n",
    "):\n",
    "    #Initialize the output and the filepaths\n",
    "    output = None \n",
    "    files = [Path(input_dir, file) for file in os.listdir(input_dir)]\n",
    "\n",
    "    #Compile the data in batches\n",
    "    for i in tqdm(range(0, len(files), batch_size)):\n",
    "        batch_files = files[i:i + batch_size]\n",
    "        batch_data = pl.concat([pl.read_parquet(file) for file in batch_files])\n",
    "        batch_grouped = batch_data.group_by('domain').agg(pl.col('count').sum())\n",
    "        if output is None:\n",
    "            output = batch_grouped\n",
    "        else:\n",
    "            output = pl.concat([output, batch_grouped]).group_by('domain').agg(pl.col('count').sum())\n",
    "    \n",
    "    #After batching, add a column with the Top Level Domain\n",
    "    output = output.with_columns(\n",
    "        pl.col(\"domain\").map_elements(lambda x: get_tld(x, fail_silently=True), return_dtype=pl.Utf8).alias(\"tld\")\n",
    "        )\n",
    "    \n",
    "    #Sort and save\n",
    "    output = output.sort('count', descending=True)\n",
    "    output.write_parquet(output_path)\n",
    "\n",
    "# Call the function\n",
    "compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Now that our data has been grouped by domain, let's see where our data is coming from. The goal of this analysis is to explore the distribution of the data by domain and by top-level domain.\n",
    "\n",
    "Then, we will select a sample to search for 'Good URLs' with high educational content.\n",
    "\n",
    "Let's start by looking at the websites that contributed the most URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>domain</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;http://lineq.jp&quot;</td><td>1346922</td></tr><tr><td>&quot;https://ameblo.jp&quot;</td><td>908818</td></tr><tr><td>&quot;http://ameblo.jp&quot;</td><td>773217</td></tr><tr><td>&quot;https://oshiete.goo.ne.jp&quot;</td><td>770550</td></tr><tr><td>&quot;https://www.amazon.co.jp&quot;</td><td>695980</td></tr><tr><td>&quot;http://mixi.jp&quot;</td><td>471151</td></tr><tr><td>&quot;http://news.livedoor.com&quot;</td><td>452297</td></tr><tr><td>&quot;https://detail.chiebukuro.yaho…</td><td>408322</td></tr><tr><td>&quot;http://q.hatena.ne.jp&quot;</td><td>386786</td></tr><tr><td>&quot;https://qa.mamari.jp&quot;</td><td>337377</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 2)\n",
       "┌─────────────────────────────────┬─────────┐\n",
       "│ domain                          ┆ count   │\n",
       "│ ---                             ┆ ---     │\n",
       "│ str                             ┆ u32     │\n",
       "╞═════════════════════════════════╪═════════╡\n",
       "│ http://lineq.jp                 ┆ 1346922 │\n",
       "│ https://ameblo.jp               ┆ 908818  │\n",
       "│ http://ameblo.jp                ┆ 773217  │\n",
       "│ https://oshiete.goo.ne.jp       ┆ 770550  │\n",
       "│ https://www.amazon.co.jp        ┆ 695980  │\n",
       "│ http://mixi.jp                  ┆ 471151  │\n",
       "│ http://news.livedoor.com        ┆ 452297  │\n",
       "│ https://detail.chiebukuro.yaho… ┆ 408322  │\n",
       "│ http://q.hatena.ne.jp           ┆ 386786  │\n",
       "│ https://qa.mamari.jp            ┆ 337377  │\n",
       "└─────────────────────────────────┴─────────┘"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'preprocessed.parquet'\n",
    "\n",
    "df = pl.read_parquet(path)\n",
    "df[['domain', 'count']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these high-frequency pages offer little to no value.\n",
    "\n",
    "- **LineQ**, a payment platform, leads in contributions with over 1 million near-duplicate pages.\n",
    "- **Ameblo** is a social media platform where fans discuss entertainment topics.\n",
    "- **Amazon** needs no introduction.\n",
    "- **Mixi** and **LiveDoor** are light news platforms heavily loaded with ads.\n",
    "\n",
    "The remaining domains—Oshiete, Yahoo Chiebukuro, Hatena, and Mamari—are Q&A platforms. \n",
    "\n",
    "Oshiete is particularly interesting, because something of their content is flagged as 'Expert'. In general, the Expert content seems to be of at least 'Minimal' to 'Basic' educational value.\n",
    "\n",
    "Here is an [example](https://oshiete.goo.ne.jp/watch/entry/aec1879357b6efac2e31a0a0a1d41307/).\n",
    "\n",
    "This kind of content is somewhat useful. There is a variety of topics, decent structure, and a healthy amount of facts. However, it does not go into topics with depth. It's better to have content that more domain specefic.\n",
    "\n",
    "In the past, I have used **Qiita**, a forum where developers share educational content with one another. The website is very well organized, with plenty of content that is targetted towards begginers. Here is an [example](https://qiita.com/ddd_nnuco/items/0873a5f286049ba46265).\n",
    "\n",
    "So, I am going to target content from **Oshiete** and **Qiita**. I already know I have a lot of pages from **Oshiete**, but I need to check if there is sufficient data for **Qitta**.\n",
    "\n",
    "The next code block shows you how to query the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>domain</th><th>count</th><th>tld</th><th>group</th></tr><tr><td>str</td><td>u32</td><td>str</td><td>i32</td></tr></thead><tbody><tr><td>&quot;https://qiita.com&quot;</td><td>68420</td><td>&quot;com&quot;</td><td>4</td></tr><tr><td>&quot;http://qiita.com&quot;</td><td>5009</td><td>&quot;com&quot;</td><td>3</td></tr><tr><td>&quot;https://qiitadon.com&quot;</td><td>927</td><td>&quot;com&quot;</td><td>2</td></tr><tr><td>&quot;https://teams.qiita.com&quot;</td><td>498</td><td>&quot;com&quot;</td><td>2</td></tr><tr><td>&quot;https://jobs.qiita.com&quot;</td><td>467</td><td>&quot;com&quot;</td><td>2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────────────────────────┬───────┬─────┬───────┐\n",
       "│ domain                  ┆ count ┆ tld ┆ group │\n",
       "│ ---                     ┆ ---   ┆ --- ┆ ---   │\n",
       "│ str                     ┆ u32   ┆ str ┆ i32   │\n",
       "╞═════════════════════════╪═══════╪═════╪═══════╡\n",
       "│ https://qiita.com       ┆ 68420 ┆ com ┆ 4     │\n",
       "│ http://qiita.com        ┆ 5009  ┆ com ┆ 3     │\n",
       "│ https://qiitadon.com    ┆ 927   ┆ com ┆ 2     │\n",
       "│ https://teams.qiita.com ┆ 498   ┆ com ┆ 2     │\n",
       "│ https://jobs.qiita.com  ┆ 467   ┆ com ┆ 2     │\n",
       "└─────────────────────────┴───────┴─────┴───────┘"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['domain'].str.contains('qiita')).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that FineWeb has plenty of data from both of the target websites. Let's start scraping URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "\n",
    "In this section, we are going to scrape links that are potentially high-quality educational content using the scraping library, **BeautifulSoup4**. We will follow this procedure:\n",
    "\n",
    "1. Visit the target website. Find the pages where the target content is indexed.\n",
    "2. Iterate through the index pages, and collect links that match the appropriate schema.\n",
    "\n",
    "Once we have the links, we can match them with the FineWeb dataframe to find the appropriate IDs.\n",
    "\n",
    "Let's start by visiting the index page of [Oshiete](https://oshiete.goo.ne.jp/watch/pro/?pg=2).\n",
    "\n",
    "The important thing to note is the structure of the URL. **Expert** pages a denoted in the path by the term **pro**.\n",
    "\n",
    "oshiete.goo.ne.jp/watch/**pro**/{page_number}.\n",
    "\n",
    "Scroll to the bottom, and you will see that there are just 22 pages. So, we can make a list to crawl with a simple list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://oshiete.goo.ne.jp/watch/pro/?pg=1',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=2',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=3',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=4',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=5',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=6',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=7',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=8',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=9',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=10',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=11',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=12',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=13',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=14',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=15',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=16',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=17',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=18',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=19',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=20',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=21',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=22']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [f'https://oshiete.goo.ne.jp/watch/pro/?pg={i}' for i in range(1, 23)]\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can click on those links to make sure they direct you to the expected page.\n",
    "\n",
    "Next, lets scrape some links from one of the target pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to scrape links from a specific page\n",
    "def scrape_links(\n",
    "    url, \n",
    "    pattern=\"\"):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        \n",
    "        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "        links = []\n",
    "\n",
    "        # Find all <a> tags and extract their href attributes\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            if pattern:\n",
    "                if re.search(pattern, href):\n",
    "                    links.append(href)\n",
    "            else:\n",
    "                links.append(href)\n",
    "\n",
    "        return links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "scrape_links(urls[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that most of those links are ones that we don't want, so we are going to need to filter them. For this, we can use regular expressions.\n",
    "\n",
    "The target pages have the path structure, '/watch/entry/{foo}/'\n",
    "\n",
    "We can use this pattern:\n",
    "r'/watch/entry/.*/$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/watch/entry/f8ee77f336084031269bf4d0cd018087/',\n",
       " '/watch/entry/f8ee77f336084031269bf4d0cd018087/',\n",
       " '/watch/entry/b2c901bb206fac02987eee47859cc177/',\n",
       " '/watch/entry/b2c901bb206fac02987eee47859cc177/',\n",
       " '/watch/entry/1e9525ca7a1e6b6d3d5725cbcbc49982/',\n",
       " '/watch/entry/1e9525ca7a1e6b6d3d5725cbcbc49982/',\n",
       " '/watch/entry/c734879afb33755571dac088d739abd7/',\n",
       " '/watch/entry/c734879afb33755571dac088d739abd7/',\n",
       " '/watch/entry/d08390b6e77798226e2f1429729ca08b/',\n",
       " '/watch/entry/d08390b6e77798226e2f1429729ca08b/',\n",
       " '/watch/entry/9286b260cfd5d0a99403e01581874f7e/',\n",
       " '/watch/entry/9286b260cfd5d0a99403e01581874f7e/',\n",
       " '/watch/entry/c66962f63d3cc5973bae94ecae28a8b8/',\n",
       " '/watch/entry/c66962f63d3cc5973bae94ecae28a8b8/',\n",
       " '/watch/entry/76a751a9754ec731a27224cceacae461/',\n",
       " '/watch/entry/76a751a9754ec731a27224cceacae461/',\n",
       " '/watch/entry/27f3e84ed646bcc13213a8d09a57f453/',\n",
       " '/watch/entry/27f3e84ed646bcc13213a8d09a57f453/',\n",
       " '/watch/entry/93cf4f7153754065be6ac96b6d93b79a/',\n",
       " '/watch/entry/93cf4f7153754065be6ac96b6d93b79a/',\n",
       " '/watch/entry/ff6c7cb777e1973503629af61d975fb4/',\n",
       " '/watch/entry/ff6c7cb777e1973503629af61d975fb4/',\n",
       " '/watch/entry/654dd3d81c4093e080074ad604941403/',\n",
       " '/watch/entry/654dd3d81c4093e080074ad604941403/',\n",
       " '/watch/entry/5bcdc37ca19ea2e0f1f5440778c5a46d/',\n",
       " '/watch/entry/5bcdc37ca19ea2e0f1f5440778c5a46d/',\n",
       " '/watch/entry/a1757358b6a8d66e31d11eaa74d25a7d/',\n",
       " '/watch/entry/a1757358b6a8d66e31d11eaa74d25a7d/',\n",
       " '/watch/entry/739cb0ff1a8000679d043f7dab5ec792/',\n",
       " '/watch/entry/739cb0ff1a8000679d043f7dab5ec792/',\n",
       " '/watch/entry/3f8bd5de6dad1e0fbe8779e0101d6c29/',\n",
       " '/watch/entry/3f8bd5de6dad1e0fbe8779e0101d6c29/',\n",
       " '/watch/entry/b39699152cc69fc04de76657a67f7836/',\n",
       " '/watch/entry/b39699152cc69fc04de76657a67f7836/',\n",
       " '/watch/entry/cd3eb2f6d64af043ce106e6030ea5840/',\n",
       " '/watch/entry/cd3eb2f6d64af043ce106e6030ea5840/',\n",
       " '/watch/entry/5432a0ee46dfb8c1b4c1952d1048af7f/',\n",
       " '/watch/entry/5432a0ee46dfb8c1b4c1952d1048af7f/',\n",
       " '/watch/entry/8738f7b33ebd420813c422a508688617/',\n",
       " '/watch/entry/8738f7b33ebd420813c422a508688617/',\n",
       " '/watch/entry/eca932309ed486f0c6c0afdb4ee0e73a/',\n",
       " '/watch/entry/eca932309ed486f0c6c0afdb4ee0e73a/',\n",
       " '/watch/entry/75e320b78f996c25b4a69566a52a1612/',\n",
       " '/watch/entry/75e320b78f996c25b4a69566a52a1612/',\n",
       " '/watch/entry/aec1879357b6efac2e31a0a0a1d41307/',\n",
       " '/watch/entry/aec1879357b6efac2e31a0a0a1d41307/',\n",
       " '/watch/entry/065ac83c6ac6ebea8afedbb5b0fe890d/',\n",
       " '/watch/entry/065ac83c6ac6ebea8afedbb5b0fe890d/',\n",
       " '/watch/entry/e53bb67d52baa5bc2e7424b3bf12e5cb/',\n",
       " '/watch/entry/e53bb67d52baa5bc2e7424b3bf12e5cb/',\n",
       " '/watch/entry/1dad5c57e8206d4609e6b5e7bf8d13f4/',\n",
       " '/watch/entry/1dad5c57e8206d4609e6b5e7bf8d13f4/',\n",
       " '/watch/entry/b8c7937c788bc01bf7da973eed06bff2/',\n",
       " '/watch/entry/b8c7937c788bc01bf7da973eed06bff2/',\n",
       " '/watch/entry/668b3e8632edf5a1e9c1062069b6fc0a/',\n",
       " '/watch/entry/668b3e8632edf5a1e9c1062069b6fc0a/',\n",
       " '/watch/entry/3c846bd79f89559d3331e9d7d5c24931/',\n",
       " '/watch/entry/3c846bd79f89559d3331e9d7d5c24931/',\n",
       " '/watch/entry/022816327913ae43ecd92582da338337/',\n",
       " '/watch/entry/022816327913ae43ecd92582da338337/',\n",
       " '/watch/entry/767b48cdeeaad83fec51f6d16e066dfb/',\n",
       " '/watch/entry/767b48cdeeaad83fec51f6d16e066dfb/',\n",
       " '/watch/entry/7b975507a26838da29ab8875a37c6a5e/',\n",
       " '/watch/entry/7b975507a26838da29ab8875a37c6a5e/',\n",
       " '/watch/entry/a67a19e477aba7b0dabc3bde692b79f9/',\n",
       " '/watch/entry/a67a19e477aba7b0dabc3bde692b79f9/',\n",
       " '/watch/entry/c689803a6d422b69774ca45007437872/',\n",
       " '/watch/entry/c689803a6d422b69774ca45007437872/',\n",
       " '/watch/entry/4f20205dc559c0a0ebba024165ea943b/',\n",
       " '/watch/entry/4f20205dc559c0a0ebba024165ea943b/']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_links(\n",
    "    url = urls[2],\n",
    "    pattern = r'/watch/entry/.*/$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, now lets script up a crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_txt(\n",
    "        _list,\n",
    "        path_to_output\n",
    "):\n",
    "    if os.path.exists(path_to_output):\n",
    "        with open(path_to_output, 'r', encoding ='utf-8') as f:\n",
    "            old_list = f.read().split('\\n')\n",
    "            _list += old_list\n",
    "    _list = list(set(_list))\n",
    "    with open(path_to_output, 'w', encoding = 'utf-8') as f:\n",
    "        f.write('\\n'.join(_list))\n",
    "\n",
    "def crawl(\n",
    "        urls,\n",
    "        pattern,\n",
    "        path_to_output\n",
    "):\n",
    "    os.makedirs(os.path.dirname(path_to_output), exist_ok=True)\n",
    "    \n",
    "    for url in tqdm(urls, desc = 'Crawling and scraping links'):\n",
    "        links = scrape_links(url, pattern)\n",
    "        if links:\n",
    "            save_list_to_txt(links, path_to_output)\n",
    "    with open(path_to_output, 'r', encoding ='utf-8') as f:\n",
    "        links = f.read().split('\\n')\n",
    "        print(f\"Crawl complete. {len(links)} links scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crawler iterates across webpages, gathers the target links, and saves the list to a text file in between each page.\n",
    "\n",
    "The function, crawl, takes the following arguments:\n",
    "\n",
    "- **urls**, a list of urls to be scraped\n",
    "- **pattern**, the regex used to filter the target links\n",
    "- **path_to_output**, where you will save the list of links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling and scraping links: 100%|██████████| 22/22 [00:17<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl complete. 753 links scraped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "crawl(\n",
    "    urls=urls,\n",
    "    pattern= r'/watch/entry/.*/$',\n",
    "    path_to_output='links/oshiete.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qiita is a bit more complicated, and I will need to scrape using Selenium. I'm going to work on a strategy and update this in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(\n",
    "        df: pl.DataFrame,\n",
    "        domain: str,\n",
    "        links: list\n",
    ") -> list:\n",
    "    # Filter rows where the 'domain' column contains the given domain\n",
    "    filtered = df.filter(df['domain'].str.contains(domain))\n",
    "    \n",
    "    # Create a pattern from the list of links\n",
    "    pattern = '|'.join(map(re.escape, links))\n",
    "    \n",
    "    # Further filter rows where the 'url' column matches the pattern\n",
    "    filtered = filtered.filter(filtered['url'].str.contains(pattern, literal=False))\n",
    "    \n",
    "    # Return the 'id' column as a list\n",
    "    return filtered['id'].to_list()\n",
    "\n",
    "def process(\n",
    "        data_dir,\n",
    "        domain,\n",
    "        links,\n",
    "        path_to_output\n",
    "):\n",
    "    os.makedirs(os.path.dirname(path_to_output), exist_ok=True)\n",
    "    paths = [Path(data_dir, file) for file in os.listdir(data_dir)]\n",
    "    for path in tqdm(paths, desc = \"Finding IDs\"):\n",
    "        df = pl.read_parquet(path)\n",
    "        ids = get_ids(df, domain, links)\n",
    "        save_list_to_txt(ids, path_to_output)\n",
    "    with open(path_to_output, 'r', encoding='utf-8') as f:\n",
    "        ids = f.read().split('\\n')\n",
    "        print(f\"IDs extracted. {len(ids)} found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding IDs: 100%|██████████| 148/148 [01:31<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs extracted. 341 found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'preprocess/stripped'\n",
    "with open('links/oshiete.txt', 'r', encoding='utf-8') as f:\n",
    "    links = f.read().split('\\n')\n",
    "\n",
    "process(\n",
    "    data_dir=data_dir,\n",
    "    domain='oshiete.txt',\n",
    "    links=links,\n",
    "    path_to_output='ids/oshiete'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Now that we have a list of IDs, we can prepare a sample for annotation with better educational density. This methodology is labor intensive, but so is data annotation. If we can find viable websites, I think it is worth the effort. To wrap things up, lets segment the corpus by its contributing domains, grouped by the number of pages those domains contributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Domains: 7,486,452\n",
      "Total URLs: 376,134,745\n",
      "shape: (7, 6)\n",
      "┌───────┬───────────┬───────────┬─────────┬───────────┬─────────────┐\n",
      "│ group ┆ group_min ┆ group_max ┆ domains ┆ pages     ┆ corpus_perc │\n",
      "│ ---   ┆ ---       ┆ ---       ┆ ---     ┆ ---       ┆ ---         │\n",
      "│ i32   ┆ u32       ┆ u32       ┆ u32     ┆ u32       ┆ f64         │\n",
      "╞═══════╪═══════════╪═══════════╪═════════╪═══════════╪═════════════╡\n",
      "│ 0     ┆ 1         ┆ 9         ┆ 4917634 ┆ 13986881  ┆ 3.72        │\n",
      "│ 1     ┆ 10        ┆ 99        ┆ 1984799 ┆ 63838337  ┆ 16.97       │\n",
      "│ 2     ┆ 100       ┆ 999       ┆ 541777  ┆ 145888934 ┆ 38.79       │\n",
      "│ 3     ┆ 1000      ┆ 9996      ┆ 40324   ┆ 85025677  ┆ 22.61       │\n",
      "│ 4     ┆ 10001     ┆ 99243     ┆ 1812    ┆ 44257590  ┆ 11.77       │\n",
      "│ 5     ┆ 100078    ┆ 908818    ┆ 105     ┆ 21790404  ┆ 5.79        │\n",
      "│ 6     ┆ 1346922   ┆ 1346922   ┆ 1       ┆ 1346922   ┆ 0.36        │\n",
      "└───────┴───────────┴───────────┴─────────┴───────────┴─────────────┘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-61f54486343c4d0a87cb7203dfa0ac7c.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-61f54486343c4d0a87cb7203dfa0ac7c.vega-embed details,\n",
       "  #altair-viz-61f54486343c4d0a87cb7203dfa0ac7c.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-61f54486343c4d0a87cb7203dfa0ac7c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-61f54486343c4d0a87cb7203dfa0ac7c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-61f54486343c4d0a87cb7203dfa0ac7c\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-b4fb3901ff329941a620a1c09ffe4abc\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"group\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pages\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_2\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-b4fb3901ff329941a620a1c09ffe4abc\": [{\"group\": 0, \"group_min\": 1, \"group_max\": 9, \"domains\": 4917634, \"pages\": 13986881, \"corpus_perc\": 3.72}, {\"group\": 1, \"group_min\": 10, \"group_max\": 99, \"domains\": 1984799, \"pages\": 63838337, \"corpus_perc\": 16.97}, {\"group\": 2, \"group_min\": 100, \"group_max\": 999, \"domains\": 541777, \"pages\": 145888934, \"corpus_perc\": 38.79}, {\"group\": 3, \"group_min\": 1000, \"group_max\": 9996, \"domains\": 40324, \"pages\": 85025677, \"corpus_perc\": 22.61}, {\"group\": 4, \"group_min\": 10001, \"group_max\": 99243, \"domains\": 1812, \"pages\": 44257590, \"corpus_perc\": 11.77}, {\"group\": 5, \"group_min\": 100078, \"group_max\": 908818, \"domains\": 105, \"pages\": 21790404, \"corpus_perc\": 5.79}, {\"group\": 6, \"group_min\": 1346922, \"group_max\": 1346922, \"domains\": 1, \"pages\": 1346922, \"corpus_perc\": 0.36}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to assign group based on the 'count' column\n",
    "def assign_group(count):\n",
    "    return int(log10(count))\n",
    "\n",
    "# Create the DataFrame and add the 'group' column\n",
    "df = pl.read_parquet('preprocessed.parquet')\n",
    "df = df.with_columns(\n",
    "    pl.col(\"count\").map_elements(lambda x: assign_group(x), return_dtype=pl.Int32).alias(\"group\")\n",
    ")\n",
    "\n",
    "# First, we calculate the total sum of 'count' for the entire dataset\n",
    "total_url_sum = df.select(pl.sum(\"count\")).to_numpy()[0][0]\n",
    "\n",
    "# Grouping and aggregating\n",
    "result = (\n",
    "    df.group_by(\"group\")\n",
    "      .agg([\n",
    "          pl.min(\"count\").alias(\"group_min\"),   # Min of 'count' within the group\n",
    "          pl.max(\"count\").alias(\"group_max\"),   # Max of 'count' within the group\n",
    "          pl.count(\"domain\").alias(\"domains\"),  # Count of domain entries per group\n",
    "          pl.sum(\"count\").alias(\"pages\")   # Sum of 'count' per group\n",
    "      ])\n",
    "      .with_columns(\n",
    "          # Adding the 'corpus_perc' column as the percentage of the total sum\n",
    "          ((pl.col(\"pages\") / total_url_sum * 100).round(2)).alias(\"corpus_perc\")\n",
    "      )\n",
    "\n",
    ").sort('group')\n",
    "\n",
    "print ('Total Domains:', f\"{result['domains'].sum():,}\")\n",
    "print ('Total URLs:', f\"{result['pages'].sum():,}\")\n",
    "print(result)\n",
    "result.plot.bar(\n",
    "    x = 'group',\n",
    "    y = 'pages'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we group the domains like this, we can see that a large portion over the corpus is contributed by very large websites. 21 million pages were contributed by just 100 websites. 45 million pages were contributed by the top ~2,000, comprising 16% of the corpus.\n",
    "\n",
    "I plan to begin by search through these large websites. My expectation is that most of them will be of no value. However, if we know we can ignore these pages, should we do another random sample, it could improve the quality. If I discover a that a few of these domains are useful, it could be a quick and effective way to improve the representation of educational material.\n",
    "\n",
    "Meanwhile, I plan to ask my Japanese friends to reccommend high quality educational websites. I don't have a specefic idea, I just plan to ask them for educational material that they would either use themselves or reccomend to others.\n",
    "\n",
    "I am going to classify them based on the system that we are already using:\n",
    "\n",
    "1. **No Educational Value**: Light news, e-commerce, sports, personal blogs, business pages, etc.\n",
    "2. **Minimal Educational Value**: Mostly amateur content. With a lot of pruning, potentially a good resource for Minimal or Basic educational content. Ex. QA forums, SEO Content Disguised as Educational Blogs.\n",
    "Examples: Quora, [Oshiete](https://oshiete.goo.ne.jp/watch/pro/), [RareJob](https://www.rarejob.com/englishlab/)\n",
    "3. **Basic Educational Value**: Mostly amateur content, but overall, education is the priority. Potentially a good resource for Minimal, Basic, or Good educational content.\n",
    "Examples: StackOverflow, [Qiita](https://qiita.com/)\n",
    "4. **Good Educational Value**: Education is clearly the priority, but their may be some issues. Maybe it covers a lot of topics, but it doesn't go into any topic in too much depth, such as WikiPedia. Maybe there is a lot of high-quality content, but also a large amount of non-educational content, such as HuggingFace.\n",
    "5. **Excellent Educational Value**: The entire website is dedicated to providing education on a certain topic, and it explores that topic in great depth. Randomly sample any page on this website, and you will likely draw one that is Good or Excellent educational value.\n",
    "Examples: [NLTK Book](https://www.nltk.org/book/), [Imabi](https://imabi.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
