{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Links and Retrieving IDs from FineWeb\n",
    "\n",
    "In this section, we are going to scrape links that are potentially high-quality educational content using the scraping libraries **BeautifulSoup4** and **Selenium**. Both strategies will follow the same basic procedure:\n",
    "\n",
    "1. Visit the target website. Find the pages where the target content is indexed.\n",
    "2. Iterate through the index pages, and collect links that match the appropriate schema.\n",
    "3. Once we have the links, match them with the FineWeb dataframe to find the appropriate IDs.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install these libraries and import these packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4 selenium polars tqdm --upgrade typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping with BeautifulSoup4\n",
    "\n",
    "BeautifulSoup4 is an easy-to-use scraping library that simplifies the process of parsing, navigating, and extracting data from HTML and XML documents.\n",
    "\n",
    "To use this libary effectively, you need to develop a strategy that is specialized for the website you are trying to scrape. This notebook provides a procedure that generalizes well to simple websites with an index page, but its effectiveness is case by case.\n",
    "\n",
    "Crucuially, this strategy assumes that target website has an **index page**, where the contents of the site are mapped out.\n",
    "\n",
    "Let's start by visiting the index page of [Oshiete](https://oshiete.goo.ne.jp/watch/pro/?pg=2).\n",
    "\n",
    "The important thing to note is the structure of the URL. From this website, we only want pages that are flagged as as **Expert**. On Oshiete, **Expert** pages are indexed seperately, denoted in the path by the term **pro**.\n",
    "\n",
    "oshiete.goo.ne.jp/watch/**pro**/{page_number}.\n",
    "\n",
    "Scroll to the bottom, and you will see that there are just 22 pages. So, we can make a list of index pages to crawl with a simple list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://oshiete.goo.ne.jp/watch/pro/?pg=1',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=2',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=3',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=4',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=5',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=6',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=7',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=8',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=9',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=10']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [f'https://oshiete.goo.ne.jp/watch/pro/?pg={i}' for i in range(1, 23)]\n",
    "urls[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you should click some of the links from the list comprehension, and confirm that they land on the target pages.\n",
    "\n",
    "The next codebox is a function that will, given a url, return all the links that are on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links_with_bs4(\n",
    "    url : str, \n",
    "    pattern: str=\"\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Scrapes links from a target page using the BeautifulSoup library.\n",
    "\n",
    "    Args:\n",
    "        url (str): The url from which to scrape links.\n",
    "        pattern (str, optional): If provided, regular expressions will be used to filter the links collected from the target page.\n",
    "\n",
    "    Returns:\n",
    "        links (list): A list of links scraped from the target page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        \n",
    "        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "        links = []\n",
    "\n",
    "        # Find all <a> tags and extract their href attributes\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            if pattern:\n",
    "                #Use regular expressions to filter links with the target pattern.\n",
    "                if re.search(pattern, href):\n",
    "                    links.append(href)\n",
    "            else:\n",
    "                links.append(href)\n",
    "\n",
    "        return links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this script, let's scrape one of the URLs and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//oshiete.goo.ne.jp/',\n",
       " 'javascript:void 0;',\n",
       " 'javascript:void 0;',\n",
       " '//oshiete.goo.ne.jp/',\n",
       " '//oshiete.goo.ne.jp/articles/qa/',\n",
       " '//oshiete.goo.ne.jp/category/list/',\n",
       " 'javascript:void 0;',\n",
       " 'javascript:void 0;',\n",
       " 'javascript:void 0;',\n",
       " 'javascript:void 0;']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_links_with_bs4(urls[0])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper does it's job, but most of the links are not useful. Using the argument, **pattern**, we can filter the links using regular expressions.\n",
    "\n",
    "To find a pattern, click on one of the links you are trying to scrape. Let's look at an example:\n",
    "\n",
    "https://oshiete.goo.ne.jp/watch/entry/89078c9390f15a9fae58d085c8091e8d/\n",
    "\n",
    "The pattern is in the path:\n",
    "\n",
    "**watch/entry/{foo}**\n",
    "\n",
    "We can target that pattern with this regex:\n",
    "\n",
    "\"watch/entry/.+\"\n",
    "\n",
    "- **r** signals that the contents of the string are to be treated as raw text\n",
    "- **watch/entry/** matches the target path that follows the top-level domain\n",
    "- **.+** matches with any character\n",
    "\n",
    "Let's try scaping links with that regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/watch/entry/c4f4a51672ca7851d0e8dcff780801f8/',\n",
       " '/watch/entry/c4f4a51672ca7851d0e8dcff780801f8/',\n",
       " '/watch/entry/4b1b58c2fce51cc54e9333783ff469dc/',\n",
       " '/watch/entry/4b1b58c2fce51cc54e9333783ff469dc/',\n",
       " '/watch/entry/7e5c41b9790a3945d253b26cf818e696/',\n",
       " '/watch/entry/7e5c41b9790a3945d253b26cf818e696/',\n",
       " '/watch/entry/53fa05ab24009c8454469ee8fcf75427/',\n",
       " '/watch/entry/53fa05ab24009c8454469ee8fcf75427/',\n",
       " '/watch/entry/72412ec44e50a29d28ec7d34a03d70f5/',\n",
       " '/watch/entry/72412ec44e50a29d28ec7d34a03d70f5/']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_links_with_bs4(\n",
    "    url=urls[0],\n",
    "    pattern='watch/entry/.+'\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. If you combine any one of those paths with the base, oshiete.goo.ne.jp, it will take you to an article that is flagged as an expert.\n",
    "\n",
    "The manner by which links are encoded varies from site to site, so you need to do some experimentation before you start scraping.\n",
    "\n",
    "Now, we have a list of URLs to scrape, and we have a method for scraping the exact links that we want. Let's set up a crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_txt(\n",
    "        _list: List[str],\n",
    "        path_to_output: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Provided with a list and file_path, this function will save a list to a text file.\n",
    "    Items in the list will be seperated by newlines.\n",
    "\n",
    "    Args:\n",
    "        _list (list): List of strings.\n",
    "        path_to_output: Path where the list is to be saved.\n",
    "    \"\"\"\n",
    "    #As the crawl iterates across pages, scraped links are saved to a text file.\n",
    "    if os.path.exists(path_to_output):\n",
    "        with open(path_to_output, 'r', encoding ='utf-8') as f:\n",
    "\n",
    "            #Combines new and old contents.\n",
    "            old_list = f.read().split('\\n')\n",
    "            _list += old_list\n",
    "    \n",
    "    #Eliminates duplicates.\n",
    "    _list = list(set(_list))\n",
    "\n",
    "    #Writes the list to a text file, sepearted by newlines.\n",
    "    with open(path_to_output, 'w', encoding = 'utf-8') as f:\n",
    "        f.write('\\n'.join(_list))\n",
    "\n",
    "def crawl(\n",
    "        scrape_method,\n",
    "        urls : List[str],\n",
    "        pattern: str,\n",
    "        path_to_output: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Given a list of URLs, this functional will iterate across pages, scrape links, and save the list to a .txt file.\n",
    "\n",
    "    Args:\n",
    "        scrape_method (function): Identifies which library to use for scraping.\n",
    "        urls (List[str]): List of URLs to scrape.\n",
    "        pattern (str): Regex used to filter URLs.\n",
    "        path_to_output (str): Path to save the list of links.\n",
    "    \"\"\"\n",
    "    \n",
    "    #If needed, creates a directory to save links.\n",
    "    os.makedirs(os.path.dirname(path_to_output), exist_ok=True)\n",
    "    \n",
    "    #Iterates across URLs, scrapes links, and saves them.\n",
    "    for url in tqdm(urls, desc = 'Crawling and scraping links'):\n",
    "        links = scrape_method(url, pattern)\n",
    "        if links:\n",
    "            save_list_to_txt(links, path_to_output)\n",
    "    \n",
    "    #Upon completion, states how many links were scraped.\n",
    "    with open(path_to_output, 'r', encoding ='utf-8') as f:\n",
    "        links = f.read().split('\\n')\n",
    "        print(f\"Crawl complete. {len(links)} links scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined methods for crawling and saving links, lets try crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9516c0e403d3451c80de353674ed7779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Crawling and scraping links:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl complete. 769 links scraped\n"
     ]
    }
   ],
   "source": [
    "crawl(\n",
    "    scrape_method= scrape_links_with_bs4,\n",
    "    urls = [f'https://oshiete.goo.ne.jp/watch/pro/?pg={i}' for i in range(1, 23)],\n",
    "    pattern = 'watch/entry/.+',\n",
    "    path_to_output = 'links/oshiete.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve IDs from FineWeb\n",
    "\n",
    "Now that we have a list of links, it's time to query the FineWeb. If the links are present, we will extract the ID.\n",
    "\n",
    "There is a lot of variation in how links are presented on-page, so we need to define a function to normalize the links.\n",
    "\n",
    "We will normalize the links by extracting the path. So, whether the link is:\n",
    "- https://www.domain.tld/this/is/the/path\n",
    "- http://www.domain.tld/this/is/the/path\n",
    "- www.domain.tld/this/is/the/path/\n",
    "- domain.tld/this/is/the/path/\n",
    "\n",
    "The function will return:\n",
    "\n",
    "- **this/is/the/path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes a given url by extracting the path and removing the terminal '/'.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL or path string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted path, or an empty string if no path is found.\n",
    "    \"\"\"\n",
    "\n",
    "    #Removes the terminal '/'\n",
    "    if url[-1] == '/':\n",
    "        url = url[:-1]\n",
    "\n",
    "    # Check if the input is a full URL. If so, extracts the path.\n",
    "    if re.match(r'^(https?:\\/\\/|www\\.)', url):\n",
    "        parsed_url = urlparse(url)\n",
    "        return parsed_url.path if parsed_url.path else '/'\n",
    "    \n",
    "    # If not a full URL, simply returns the input.\n",
    "    else:\n",
    "        return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a function to combine our list of links into a regex for efficient querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_links_into_regex(links: List[str]) -> str:\n",
    "    \"\"\"Given a list of strings, this will produce a regex for querying FineWeb.\"\"\"\n",
    "    \n",
    "    return '|'.join(map(re.escape, links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define a function to query a single shard of FineWeb, and then test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<urn:uuid:9f04ee84-c012-4947-93a6-7d545ed8ff89>',\n",
       " '<urn:uuid:cd26f36c-88e2-434b-a592-7536c1640aea>',\n",
       " '<urn:uuid:9263920e-e830-4331-bf21-2b1b465bd4eb>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ids(\n",
    "        df: pl.DataFrame,\n",
    "        domain: str,\n",
    "        links: List[str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Given a polars DataFrame, filter rows that match one of the target links, and return a list of IDs.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): DataFrame to filter.\n",
    "        domain (str): Target domain.\n",
    "        links (List[str]): List of promising links from the target domain.\n",
    "\n",
    "    Returns:\n",
    "        List of IDs from FineWeb tahtt align with the target links.\n",
    "    \"\"\"\n",
    "\n",
    "    #Normalize the links.\n",
    "    links = [extract_path(link) for link in links]\n",
    "\n",
    "    #Combine the links.\n",
    "    pattern = combine_links_into_regex(links)\n",
    "\n",
    "    #Filter the DataFrame for rows that contain the target domain.\n",
    "    filtered = df.filter(df['domain'].str.contains(domain))\n",
    "\n",
    "    #From the filtered DataFrame, filter rows the URL matches one of the target links.\n",
    "    filtered = filtered.filter(filtered['url'].str.contains(pattern, literal=False))\n",
    "\n",
    "    #Return a list of IDs from the filtered rows.\n",
    "    return filtered['id'].to_list()\n",
    "\n",
    "\n",
    "#Now, let's test the function.\n",
    "\n",
    "domain = 'oshiete'\n",
    "\n",
    "with open('links/oshiete.txt', 'r', encoding='utf-8') as f:\n",
    "    links = f.read().split('\\n')\n",
    "\n",
    "data_dir = 'preprocess/stripped'\n",
    "df = pl.read_parquet(Path(data_dir, os.listdir(data_dir)[0]))\n",
    "\n",
    "get_ids(\n",
    "    df=df,\n",
    "    domain=domain,\n",
    "    links=links\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define a pipe line that will iterate through FineWeb files, extract links, and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_retrieval_pipeline(\n",
    "        data_dir: str,\n",
    "        domain: str,\n",
    "        links: List[str],\n",
    "        path_to_output: str\n",
    ")-> None:\n",
    "    \"\"\"\n",
    "    Given a domain, a list of links, and a directory where files are saved,\n",
    "    this pipeline will retrieve IDs from a series of FineWeb files.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to directory where FineWeb files are saved.\n",
    "        domain: Target domain for ID extraction.\n",
    "        links: List of target links for ID extraction.\n",
    "        path_to_output: Path to save list of IDs.\n",
    "    \"\"\"\n",
    "    #If neccesary, makes a directory to save list of IDs.\n",
    "    os.makedirs(os.path.dirname(path_to_output), exist_ok=True)\n",
    "\n",
    "    #Build a list of filepaths.\n",
    "    paths = [Path(data_dir, file) for file in os.listdir(data_dir)]\n",
    "\n",
    "    #Iterates through files, extracts IDs, and saves them.\n",
    "    for path in tqdm(paths, desc = \"Finding IDs\"):\n",
    "        df = pl.read_parquet(path)\n",
    "        ids = get_ids(df, domain, links)\n",
    "        save_list_to_txt(ids, path_to_output)\n",
    "    \n",
    "    #Completes the process by returning the number of IDs matched.\n",
    "    with open(path_to_output, 'r', encoding='utf-8') as f:\n",
    "        ids = f.read().split('\\n')\n",
    "        print(f\"IDs extracted. {len(ids)} found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0527941a046543a6af087f0b385bdb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding IDs:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs extracted. 341 found.\n"
     ]
    }
   ],
   "source": [
    "id_retrieval_pipeline(\n",
    "    data_dir='preprocess/stripped',\n",
    "    domain='oshiete',\n",
    "    links=links,\n",
    "    path_to_output='ids/oshiete.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping with Selenium\n",
    "\n",
    "Selenium is a more flexible library because, unlike BeutifulSoup4, it can interact with web pages as a normal user. Therefore, it can deal with things like lazy loading and Javascript.\n",
    "\n",
    "In the case of **Qiita**, the article retrieval system is scripted in JavaScript, so BeautifulSoup4 is ineffective.\n",
    "\n",
    "Although it is more flexible, it is also much slower. Nevertheless, it is quite easy to use.\n",
    "\n",
    "Just like before, let's start by visiting the **Qiita** index [page](https://qiita.com/search).\n",
    "\n",
    "In the search bar, click on the icon on the right, and it provides a list of search parameters. We are going to use these ones:\n",
    "- **tag**: 初心者 (beginner)\n",
    "- **created**: <=2024-04-24 (the date of the most recent article in Japanese FineWeb)\n",
    "- **sort**: by Likes, we are assuming the articles with more Likes are higher quality\n",
    "\n",
    "Plug in the query parameters, and the URL looks like this.\n",
    "\n",
    "https://qiita.com/search?q=tag%3A%E5%88%9D%E5%BF%83%E8%80%85&sort=like&stocked=&page=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a function to scrape URLs with Selenium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links_with_selenium(\n",
    "    url: str,\n",
    "    pattern: str,\n",
    "    driver: object = None\n",
    "    )-> List[str]:\n",
    "    \"\"\"\n",
    "    Given a URL, scrape links using Selenium.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL from which to scrape links.\n",
    "        pattern (str): Regex to match target links.\n",
    "        driver (object): Web browser for scraping links. Defaults to Chrome.\n",
    "    \n",
    "    Returns:\n",
    "        List of links that matches the input regex.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    if not driver:\n",
    "        driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Locate all links on the page using the <a> tag\n",
    "    links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "    # Extract the href attribute of each link\n",
    "    for link in links:\n",
    "        href = link.get_attribute(\"href\")\n",
    "        if href:  # Ensure href is not None\n",
    "            if pattern:\n",
    "                if re.search(pattern, href):\n",
    "                    output.append(href)\n",
    "            else:\n",
    "                output.append(href)\n",
    "\n",
    "    # Print the collected links\n",
    "    driver.quit()\n",
    "    return list(set(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, the strategy is the same. Prepare a list of links to crawl and pick out a pattern for you target pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://qiita.com/bo_zu_/items/88f45b132c8293dcd9b1',\n",
       " 'https://qiita.com/shimajiri/items/501828dc8d589e214470',\n",
       " 'https://qiita.com/JunyaShibato/items/3aa5f7f3fc991de17f3f',\n",
       " 'https://qiita.com/0xfffffff7/items/028ff8c920a6a8c67dc5',\n",
       " 'https://qiita.com/jesus_isao/items/63557eba36819faa4ad9',\n",
       " 'https://qiita.com/zamis/items/703bfcea027a70c1cec6',\n",
       " 'https://qiita.com/yasuoyasuo/items/c43783316a4d141a140f',\n",
       " 'https://qiita.com/suzu-4/items/ea5d802cb0ad16682ae2',\n",
       " 'https://qiita.com/YudaiTsukamoto/items/42a8df22ca4c6b327dfd',\n",
       " 'https://qiita.com/soyanchu/items/d1cb9785fc211941a009']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare a list of URLs to crawl.\n",
    "urls = [f\"https://qiita.com/search?q=tag%3A%E5%88%9D%E5%BF%83%E8%80%85%20created%3A%3C%3D2024-04-25&sort=like&stocked=&page={i}\"\n",
    "        for i in range(1, 101)]\n",
    "\n",
    "#Define your regex.\n",
    "pattern = \".com/.+/items/.+$\"\n",
    "\n",
    "#Test out the function.\n",
    "scrape_links_with_selenium(\n",
    "    url=urls[0],\n",
    "    pattern=\".com/.+/items/.+$\"\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything works, so let's crawl. This is going to take quite a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb66e21b291a4c7c951b77a371955818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Crawling and scraping links:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl complete. 2007 links scraped\n"
     ]
    }
   ],
   "source": [
    "crawl(\n",
    "    scrape_method=scrape_links_with_selenium,\n",
    "    urls = urls,\n",
    "    pattern = pattern,\n",
    "    path_to_output='links/qiita.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's extract IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106120663a76438e9b03b39640d4146d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding IDs:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs extracted. 585 found.\n"
     ]
    }
   ],
   "source": [
    "data_dir='preprocess/stripped'\n",
    "domain='qiita'\n",
    "with open('links/qiita.txt', 'r', encoding='utf-8') as f:\n",
    "    links = f.read().split('\\n')\n",
    "path_to_output = 'ids/qiita.txt'\n",
    "\n",
    "id_retrieval_pipeline(\n",
    "    data_dir=data_dir,\n",
    "    domain=domain,\n",
    "    links=links,\n",
    "    path_to_output=path_to_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With the list of IDs in hand, we can now prepare a sample for annotation that prioritizes better educational density. While this methodology requires significant effort, it is far less labor-intensive than data annotation itself.\n",
    "\n",
    "Data annotation is a monotonous task, and the attention of our annotators is a valuable resource. By implementing strategies for pre-screening data, we can optimize this process and achieve greater efficiency in the long run.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
