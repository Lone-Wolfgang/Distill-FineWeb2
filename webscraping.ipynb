{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Links and Retrieving IDs from FineWeb\n",
    "\n",
    "In this section, we are going to scrape links that are potentially high-quality educational content using the scraping libraries **BeautifulSoup4** and **Selenium**. Both strategies will follow the same basic procedure:\n",
    "\n",
    "1. Visit the target website. Find the pages where the target content is indexed.\n",
    "2. Iterate through the index pages, and collect links that match the appropriate schema.\n",
    "3. Once we have the links, match them with the FineWeb dataframe to find the appropriate IDs.\n",
    "\n",
    "In the previous notebook, **preprocess.ipynb**, the Japanese segment of FineWeb was grouped by domain and combined in a single DataFrame. From that point, it was very easy to figure out which websites have contributed a lot of data.\n",
    "\n",
    "This notebook will demonstrate how to scrape two websites:\n",
    "\n",
    "- **Oshiete** is a QA forum. This website stood out because it has articles that are marked as [expert](https://oshiete.goo.ne.jp/watch/entry/32a0ba00f95212ae379a105f4a09ed36/), which are overall higher quality. I think this will be a good source of Minimally and Basic Educational material on some general topics.\n",
    "- **Qiita** is a popular Japanese developers' [forum](https://qiita.com/U-MA/items/896c49d46585e32ff7b1). This seems to be a good source of Basic and Good Educational material within the domain of programming and web development.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already, pleaes install the library, **fineweb_tools**. All of the neccesary dependencies will be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fineweb_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping with BeautifulSoup4\n",
    "\n",
    "BeautifulSoup4 is an easy-to-use scraping library that simplifies the process of parsing, navigating, and extracting data from HTML and XML documents.\n",
    "\n",
    "To use this libary effectively, you need to develop a strategy that is specialized for the website you are trying to scrape. This notebook provides a procedure that generalizes well to simple websites with an index page, but its effectiveness is case by case.\n",
    "\n",
    "Crucuially, this strategy assumes that target website has an **index page**, where the contents of the site are mapped out.\n",
    "\n",
    "Let's start by visiting the index page of [Oshiete](https://oshiete.goo.ne.jp/watch/pro/?pg=2).\n",
    "\n",
    "The important thing to note is the structure of the URL. From this website, we only want pages that are flagged as as **Expert**. On Oshiete, **Expert** pages are indexed seperately, denoted in the path by the term **pro**.\n",
    "\n",
    "oshiete.goo.ne.jp/watch/**pro**/{page_number}.\n",
    "\n",
    "Scroll to the bottom, and you will see that there are just 22 pages. So, we can make a list of index pages to crawl with a simple list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://oshiete.goo.ne.jp/watch/pro/?pg=1',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=2',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=3',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=4',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [f'https://oshiete.goo.ne.jp/watch/pro/?pg={i}' for i in range(1, 23)]\n",
    "urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you should click some of the links from the list comprehension, and confirm that they land on the target pages.\n",
    "\n",
    "Let's try scraping one of these pages. Use the function, **scrape_links**. In this demonstration, we will input the required arguments:\n",
    "- **method**: Which library to use for scraping, either **bs4** or **selenium**.\n",
    "- **url**: The URL from which to scrape links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " '/login?callback_action=login_or_signup&redirect_to=%2Fsearch%3Fq%3Dtag%253A%25E5%2588%259D%25E5%25BF%2583%25E8%2580%2585%2520created%253A%253C%253D2024-04-25%26sort%3Dlike%26stocked%3D%26page%3D1&realm=qiita',\n",
       " '/signup?callback_action=login_or_signup&redirect_to=%2Fsearch%3Fq%3Dtag%253A%25E5%2588%259D%25E5%25BF%2583%25E8%2580%2585%2520created%253A%253C%253D2024-04-25%26sort%3Dlike%26stocked%3D%26page%3D1&realm=qiita',\n",
       " '/',\n",
       " '/question-feed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fineweb_tools.webscrape import scrape_links\n",
    "\n",
    "scrape_links(\n",
    "    method='bs4',\n",
    "    url=urls[0]\n",
    ")[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper does it's job, but most of the links are not useful. Using the argument, **regex**, we can filter the links using regular expressions.\n",
    "\n",
    "To forumlate a **regex**, click on one of the links you are trying to scrape. Let's look at an example:\n",
    "\n",
    "https://oshiete.goo.ne.jp/watch/entry/89078c9390f15a9fae58d085c8091e8d/\n",
    "\n",
    "The pattern is in the path:\n",
    "\n",
    "**watch/entry/{foo}**\n",
    "\n",
    "We can target that pattern with this regex:\n",
    "\n",
    "**r\"watch/entry/.+\"**\n",
    "\n",
    "- **r** signals that the contents of the string are to be treated as raw text\n",
    "- **watch/entry/** matches the target path that follows the top-level domain\n",
    "- **.+** matches with any character, as long as there are 1 or more characters\n",
    "\n",
    "Let's try scaping links with that regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/watch/entry/53fa05ab24009c8454469ee8fcf75427/?from=entry_side_new',\n",
       " '/watch/entry/4786777415b37917a79acfa64327b629/',\n",
       " '/watch/entry/997ca7db83e4da7314d3fc491fc3167a/',\n",
       " '/watch/entry/5a038db44f5e70d6a23fbf814bace574/?from=entry_side_rank',\n",
       " '/watch/entry/72412ec44e50a29d28ec7d34a03d70f5/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_links(\n",
    "    method = 'bs4',\n",
    "    url=urls[0],\n",
    "    regex='watch/entry/.+'\n",
    ")[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. If you combine any one of those paths with the base, oshiete.goo.ne.jp, it will take you to an article that is flagged as an expert.\n",
    "\n",
    "The manner by which links are encoded varies from site to site, so you need to do some experimentation before you start scraping.\n",
    "\n",
    "Next, let's try crawling. The function **crawl** takes these arguments:\n",
    "- **method**: Scraping library, **bs4** or **selenium**.\n",
    "- **urls**: A list of URLs to scrape.\n",
    "- **output_path**: The path to save your list of scraped links.\n",
    "- **regex**: The regular expression used to filter scraped links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf9c59a0d7a4b339f3d48d20af1e5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Crawling and scraping links:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl complete. 774 links scraped\n"
     ]
    }
   ],
   "source": [
    "from fineweb_tools.webscrape import crawl\n",
    "\n",
    "crawl(\n",
    "    method='bs4',\n",
    "    urls=urls,\n",
    "    output_path='links/oshiete.txt',\n",
    "    regex='watch/entry/.+'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve IDs from FineWeb\n",
    "\n",
    "Now that we have a list of IDs, let's find out if they are contained in FineWeb and extract the corresponding IDs.\n",
    "\n",
    "The **id_retrieval_pipeline** takes four arguments:\n",
    "- **data_dir**: Path to FineWeb data. I reccommend that you used the **stripped data** from from preprocessing because it is more efficient. This the pipeline works fine on raw data too.\n",
    "- **domain**: The domain name from which you scraped links. Used for initial filtering.\n",
    "- **links_path**: Path to the text file where links are saved.\n",
    "- **output_path**: Path to where the list of IDs is to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2717d958784675813e6c3f8930d6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding IDs:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs extracted. 341 found.\n"
     ]
    }
   ],
   "source": [
    "from fineweb_tools.webscrape import id_retrieval_pipeline\n",
    "\n",
    "id_retrieval_pipeline(\n",
    "    data_dir='intermediate/stripped/jpn_Jpan',\n",
    "    domain='oshiete',\n",
    "    links_path='links/oshiete.txt',\n",
    "    output_path='ids/oshiete.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping with Selenium  \n",
    "\n",
    "Selenium is a more flexible library because, unlike BeautifulSoup4, it can interact with web pages as a normal user. Therefore, it can handle things like lazy loading and JavaScript.  \n",
    "\n",
    "In the case of **Qiita**, the article retrieval system is scripted in JavaScript, so BeautifulSoup4 is ineffective.  \n",
    "\n",
    "Although it is more flexible, Selenium is also much slower. Nevertheless, it is quite easy to use.  \n",
    "\n",
    "Just like before, let's start by visiting the **Qiita** index [page](https://qiita.com/search).  \n",
    "\n",
    "In the search bar, click on the icon on the right, which provides a list of search parameters. We are going to use the following ones:  \n",
    "- **tag**: 初心者 (beginner), which becomes '%3A%E5%88%9D%E5%BF%83%E8%80%85' after URL encoding.\n",
    "- **created**: <=2024-04-24 (the date of the most recent article in Japanese FineWeb)  \n",
    "- **sort**: by Likes (we are assuming articles with more Likes are of higher quality)  \n",
    "\n",
    "\n",
    "Plug in the query parameters, and the URL looks like this.\n",
    "\n",
    "https://qiita.com/search?q=tag%3A%E5%88%9D%E5%BF%83%E8%80%85&sort=like&stocked=&page=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a function to scrape URLs with Selenium:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, the strategy is the same. Prepare a list of links to crawl and pick out a pattern for you target pages.\n",
    "\n",
    "Note that, when using selenium, you need to consider your **driver**, which is the web browser used to interact with pages.\n",
    "\n",
    "It defaults to Chrome, but you can change it with the argument, **driver**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://qiita.com/jesus_isao/items/63557eba36819faa4ad9',\n",
       " 'https://qiita.com/zamis/items/703bfcea027a70c1cec6',\n",
       " 'https://qiita.com/kazuo_reve/items/d1a3f0ee48e24bba38f1',\n",
       " 'https://qiita.com/shimajiri/items/501828dc8d589e214470',\n",
       " 'https://qiita.com/jnchito/items/dedb3b889ab226933ccf',\n",
       " 'https://qiita.com/m-yamashita/items/889c116b92dc0bf4ea7d',\n",
       " 'https://qiita.com/rana_kualu/items/379eefb3a40c6b44cb92',\n",
       " 'https://qiita.com/nesheep5/items/e7196ba496e59bb2aa28',\n",
       " 'https://qiita.com/0xfffffff7/items/028ff8c920a6a8c67dc5',\n",
       " 'https://qiita.com/karamage/items/771b633c3243989418a2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fineweb_tools.webscrape import scrape_links\n",
    "\n",
    "#Prepare a list of URLs to crawl.\n",
    "urls = [f\"https://qiita.com/search?q=tag%3A%E5%88%9D%E5%BF%83%E8%80%85%20created%3A%3C%3D2024-04-25&sort=like&stocked=&page={i}\"\n",
    "        for i in range(1, 10)]\n",
    "\n",
    "#Define your regex.\n",
    "pattern = \".com/.+/items/.+$\"\n",
    "\n",
    "#Test out the function.\n",
    "scrape_links(\n",
    "    method='selenium',\n",
    "    url=urls[0],\n",
    "    regex=\".com/.+/items/.+$\"\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything works, so let's crawl. This is going to take quite a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c89624012e341ba9cde1c76c6c238d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Crawling and scraping links:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl complete. 178 links scraped\n"
     ]
    }
   ],
   "source": [
    "from fineweb_tools.webscrape import crawl\n",
    "\n",
    "crawl(\n",
    "    method='selenium',\n",
    "    urls = urls,\n",
    "    output_path = 'links/qiita.txt',\n",
    "    regex = \".com/.+/items/.+$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's extract IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b162b284f4b453da4807646b640b4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding IDs:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs extracted. 585 found.\n"
     ]
    }
   ],
   "source": [
    "from fineweb_tools.webscrape import id_retrieval_pipeline\n",
    "\n",
    "id_retrieval_pipeline(\n",
    "    data_dir='intermediate/stripped/jpn_Jpan',\n",
    "    domain='qiita',\n",
    "    links_path='links/qiita.txt',\n",
    "    output_path='ids/qiita.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With the list of IDs in hand, we can now prepare a sample for annotation that prioritizes better educational density. While this methodology requires significant effort, it is far less labor-intensive than data annotation itself.\n",
    "\n",
    "Data annotation is a monotonous task, and the attention of our annotators is a valuable resource. By implementing strategies for pre-screening data, we can optimize this process and achieve greater efficiency in the long run.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
