{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect IDs from FineWeb\n",
    "\n",
    "I am working on collecting IDs from the Japanese segment of FineWeb that are potentially high-quality educational content. My goal is to isolate the educational material from 50 websites that cover a variety of topics. You can track my progress here:\n",
    "\n",
    "[Japanese FineWeb IDs](https://huggingface.co/datasets/LoneWolfgang/Japanese-FineWeb-IDs)\n",
    "\n",
    "This notebook will guide you through my methods.\n",
    "\n",
    "## Structure:\n",
    "\n",
    "1. **Selecting Websites**: An overview of the strategies I used to identify websites with high-quality educational content.  \n",
    "2. **Distilling Educational Material**: How to use path structure and web scraping to isolate educational content from non-educational material.  \n",
    "3. **Retrieve IDs**: Given a list of links, retrieve corresponding IDs from the FineWeb corpus.  \n",
    "\n",
    "## Setup:\n",
    "\n",
    "Before getting started, please read through this [notebook](https://github.com/Lone-Wolfgang/Distill-FineWeb2/blob/main/preprocess.ipynb). Completing the **Quickstart** will ensure that you have installed the necessary dependencies, downloaded your FineWeb language segment, and preprocessed it for efficient browsing.\n",
    "\n",
    "## Selecting Websites\n",
    "\n",
    "This is the most labor-intensive part of the task. Educational content is sparse, and a significant portion is not represented in the FineWeb corpus. To pinpoint educational content, I have used four strategies:\n",
    "\n",
    "1. Solicit recommendations.  \n",
    "2. Query FineWeb domains using keywords.  \n",
    "3. Follow external links from educational websites.  \n",
    "4. Sift through blog ranking websites.\n",
    "\n",
    "### Solicit Recommendations\n",
    "\n",
    "This is the simplest strategy and, with sufficient community support, scales the best. Among my Japanese connections, I have contacts with expertise in web development, English education, various STEM topics, and music production. I have asked them to recommend sites with educational content that they use themselves or would recommend to others. For instance, a friend of mine recommended [Zenn](https://zenn.dev/), which hosts an assortment of user-generated articles covering a variety of topics in web development.\n",
    "\n",
    "### Query FineWeb Domains using Keywords\n",
    "\n",
    "The next simplest strategy is to search FineWeb domains using educational keywords. This method utilizes the [preprocessed](https://github.com/Lone-Wolfgang/Distill-FineWeb2/blob/main/preprocess.ipynb). To query the DataFrame, we will use the function, **fineweb_tools.analyze.filter_fn**. This function takes the arguments:\n",
    "- **path**: Path to the preprocessed DataFrame. \n",
    "- **filter_column**: Column to apply a filter.\n",
    "- **target**: Term to filter from the filter_column.\n",
    "- **retrieve_columns**: Function only returns columns speceified in the retrieve columns.\n",
    "\n",
    "Let's try searching for **rekishi**, which is the Japanese term for history:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https://rekishisuki.com', 3138)\n",
      "('https://www.rekishijin.com', 1963)\n",
      "('https://rekishijidaisakka.hatenablog.com', 1120)\n",
      "('http://rekishi-club.com', 1020)\n",
      "('https://rekishiru.site', 814)\n",
      "('https://chirirekishizukihisachan.hatenablog.com', 710)\n",
      "('http://www.rekishinoshinzui.com', 683)\n",
      "('https://rekishi-shizitsu.jp', 653)\n",
      "('http://rekishi.info', 591)\n",
      "('https://www.rekishiwales.com', 570)\n"
     ]
    }
   ],
   "source": [
    "from fineweb_tools.analyze import filter_fn\n",
    "import polars as pl\n",
    "\n",
    "result = filter_fn(\n",
    "    path = 'FineWeb/preprocessed/jpn_Jpan.parquet',\n",
    "    filter_column='domain',\n",
    "    target='rekishi',\n",
    "    retrieve_columns=['domain', 'count']\n",
    ").head(10)\n",
    "\n",
    "for row in result.iter_rows():\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this simple query, we have found three valuable resources:\n",
    "\n",
    "- [RekishiJin](https://www.rekishijin.com/) & [Rekishi Shizitsu](https://rekishi-shizitsu.jp/), which are niche Wikipedia style articles about important people and events in Japanese history\n",
    "- [RekishiWales](https://www.rekishiwales.com), which focuses on Welsh history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow External Links from Educational Websites\n",
    "\n",
    "Teachers help teachers.\n",
    "\n",
    "When querying 'biology', I discovered a blog written by a high school biology teacher. Most of their content discussed their stance on educational methods, which are not useful for FineWeb. However, they also included a section of [links](https://biologymanabiai.jimdoweb.com/%E3%83%AA%E3%83%B3%E3%82%AF%E9%9B%86/%E9%AB%98%E6%A0%A1%E7%94%9F%E7%89%A9%E6%8E%88%E6%A5%AD%E9%96%A2%E9%80%A3%E3%83%AA%E3%83%B3%E3%82%AF%E9%9B%86/) to useful educational resources.\n",
    "\n",
    "From here, I found:\n",
    "\n",
    "- [Shinrin Ringyou](https://www.shinrin-ringyou.com/): A fantastic resource about forest ecology.  \n",
    "- [Statistics Academy](https://www.stat.go.jp/naruhodo/index.html): A collection of basic statistic lessons for elementary and high school students.  \n",
    "\n",
    "These two sites are some of the best quality that I have found. In your exploration, if you do stumble upon a website that is earnestly passionate about education, look for external links before you move on.\n",
    "\n",
    "### Sift through Blog Ranking Websites\n",
    "\n",
    "A final resource for educational content is blog aggregators, such as [Blog Mura](https://blogmura.com/). This website is particularly useful because blogs are categorized and ranked based on user activity.\n",
    "\n",
    "This is a valuable resource for niche information that you would not find in a typical educational curriculum, such as:  \n",
    "- [Ace Compliance](https://www.ace-compliance.com/blog/): Explores the legal code directing waste management practices and explains it in common-sense terminology.  \n",
    "- [KamiConsal](https://kamiconsal.jp/): All things paperâ€”manufacturing, use-cases, and trivia.  \n",
    "- [AgriNavi](https://agri.mynavi.jp): Dedicated to disseminating environmentally friendly agricultural practices.  \n",
    "\n",
    "Although this stragegy is useful for accessing resources you would otherwise overlook there are significant limitations:\n",
    "\n",
    "- **Recency**: Blogs are typically ranked by recent actitivity, but the FineWeb corpus was collected about a year ago. Many of the top-ranked blogs are **not** represented in the FineWeb corpus. If you find a promising resource, start by querying the domain using **fineweb_tools.analyze.filter_fn**.\n",
    "\n",
    "- **SEO Dominanace**: Many of the top ranking blogs are intended to promote some product or service. Although they are educationally presenting, their content is usually underwhelming.\n",
    "\n",
    "- **Credibility**: Unlike professional resources, amateur blogs are not obligated to share their sources. It is difficult to vet resources, so please procede with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilling Educational Content\n",
    "\n",
    "Regardless of their educational quality, all websites have at least some non-educational content, such as category pages or an 'About Me' section. Many sites have excellent quality material, but only on a minority of pages. To improve the sample quality, it is important to sift educational from non-educational material.\n",
    "\n",
    "This section of the notebook presents three strategies for distilling links to pages with educational content. After collecting links, the final section of the notebook explains how to retrieve IDs from the FineWeb corpus.\n",
    "\n",
    "- **Use Path Branches**: When possible, this is the preferred method. Sometimes, the type of content is denoted in the structure of the URL. In this case, it is very easy to target educational content with regular expressions.\n",
    "\n",
    "- **Scrape Links Using BeautifulSoup4**: When the type of content is not reflected in the link structure, it is better to crawl index pages with educational content and scrape links. BeautifulSoup4 (bs4) is the simplest method for web scraping.\n",
    "\n",
    "- **Scrape Links Using Selenium**: More advanced websites use JavaScript with query parameters to dynamically retrieve pages based on user requests. BS4 is not effective for this type of website. Selenium is a powerful library that interacts with webpages as a human-user, but it is much slower.\n",
    "\n",
    "### Use Path Branches\n",
    "\n",
    "This is an effective strategy when the majority of website content is educational, or when site is well-structured and content type is clearly reflected in the URL.\n",
    "\n",
    "To demonstrate, we are going to use [Mathematica](https://mathematica.site/), a website dedicated to the ancient origins of mathematical theory.\n",
    "\n",
    "We will start by collecting a list of links from mathematica that are in the FineWeb Corpus. We will use **fineweb_tools.analyze.filter_fineweb**. This functions iterates through FineWeb files, applies **fineweb_tools.analyze.filter_fn**, and collects the results into a single DataFrame.\n",
    "\n",
    "**filter_fineweb** takes two arguments:\n",
    "- **data_dir**: Where the FineWeb files are saved. I usee the stripped files from the intermediate folder, but raw files will work too.\n",
    "- **filter_fn**: The custom filtering function.\n",
    "\n",
    "As before, **filter_fn** takes four arguments:\n",
    "- **path**: Input as a lambda argument.\n",
    "- **filter_column**: The column to filter.\n",
    "- **target**: The string to locate in the filter column.\n",
    "- **retrieve_columns**: The columns returned in the final DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d5b4de7d3841e8aad795f0ef558d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering FineWeb:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['https://mathematica.site/keyword-term/egyptian-fraction/',\n",
       " 'https://mathematica.site/web-mag/web-mag-egypt/3-5/',\n",
       " 'https://mathematica.site/newsall/news/renewal-info/',\n",
       " 'https://mathematica.site/keyword-term/mastaba/',\n",
       " 'https://mathematica.site/keyword-term/neftis-2/']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fineweb_tools.analyze import filter_fineweb, filter_fn\n",
    "\n",
    "filtered = filter_fineweb(\n",
    "    data_dir = 'FineWeb/intermediate/jpn_Jpan/stripped', #\n",
    "    filter_fn = lambda path: filter_fn(\n",
    "        path=path,\n",
    "        filter_column='domain',\n",
    "        target = 'mathematica.site',\n",
    "        retrieve_columns=['domain', 'url']\n",
    "    )\n",
    ")\n",
    "\n",
    "#Create a list with the URLs\n",
    "urls = filtered['url'].to_list()\n",
    "urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the initial list of URLs, you can begin to see the link structure.\n",
    "\n",
    "To get a better overview, use the function, **fineweb_tools.collect_ids.count_path_branches**, which recursively splits paths into branches and counts them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/web-mag', 93),\n",
       " ('/keyword-term', 29),\n",
       " ('/web-mag/web-mag-egypt', 23),\n",
       " ('/web-mag/calendar', 17),\n",
       " ('/web-mag/column', 16)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fineweb_tools.collect_ids import count_path_branches\n",
    "\n",
    "count_path_branches(urls).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When viewd this way, you can see that the most common path is **web-mag**, which is where educational content is presented.\n",
    "\n",
    "Let's use a list comprehension with a regular expression to filter the target links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://mathematica.site/web-mag/web-mag-egypt/3-5/',\n",
       " 'https://mathematica.site/web-mag/web-mag-egypt/1-3/',\n",
       " 'https://mathematica.site/web-mag/web-mag-egypt/1-2/',\n",
       " 'https://mathematica.site/web-mag/euclid/euclid1-1/',\n",
       " 'https://mathematica.site/web-mag/web-mag-babylonian/invention-of-numbers-8/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fineweb_tools.collect_ids import save_list_to_txt\n",
    "import re\n",
    "\n",
    "#targets strings starting with 'web-mag/' followed by at least 1 of any character\n",
    "regex = 'web-mag/.+'\n",
    "\n",
    "#filters urls using a list comprehension with re.search\n",
    "target = [url for url in urls if re.search(regex, url)]\n",
    "\n",
    "#saves the extracted list to a text file for id extraction\n",
    "save_list_to_txt(target, output_path='hub_dataset/jpn_Jpan/links.mathematica.txt')\n",
    "\n",
    "target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The links were saved to a text file, and we will retrieve the corresponding IDs in the final section.\n",
    "\n",
    "When website structure is not reflected in the URL, you need to use webscraping to isolate educational content. We will start by using BeautifulSoup4 (BS4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping with BeautifulSoup4\n",
    "\n",
    "BeautifulSoup4 is an easy-to-use scraping library that simplifies the process of parsing, navigating, and extracting data from HTML and XML documents.\n",
    "\n",
    "To demonstrate, we will use the QA forum, Oshiete.\n",
    "\n",
    "Oshiete is a QA forum similar to Quora, and it covers a lot of different topics. Most of it is not useful, but some articles are tagged as Expert, meaning that the author has some authority.\n",
    "\n",
    "Let's start by visiting the index page of expert articles on [Oshiete](https://oshiete.goo.ne.jp/watch/pro/?pg=2).\n",
    "\n",
    "The important thing to note is the structure of the URL. **Expert** pages are indexed seperately, denoted in the path by the term **pro**.\n",
    "\n",
    "oshiete.goo.ne.jp/watch/**pro**/{page_number}.\n",
    "\n",
    "Scroll to the bottom, and you will see that there are just 22 pages. So, we can make a list of index pages to crawl with a simple list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://oshiete.goo.ne.jp/watch/pro/?pg=1',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=2',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=3']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [f'https://oshiete.goo.ne.jp/watch/pro/?pg={i}' for i in range(1, 23)]\n",
    "urls[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, build a list of pages using the function **fineweb_tools.collect_ids.pagerange**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://oshiete.goo.ne.jp/watch/pro/?pg=1',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=2',\n",
       " 'https://oshiete.goo.ne.jp/watch/pro/?pg=3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fineweb_tools.collect_ids import pagerange\n",
    "\n",
    "#Handy when combining lists from many different index pages\n",
    "urls = pagerange('https://oshiete.goo.ne.jp/watch/pro/?pg={}', first = 1, last = 23)\n",
    "urls[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you should click some of the links from the list comprehension, and confirm that they land on the target pages.\n",
    "\n",
    "Let's try scraping one of these pages. The function **fineweb_tools.collect_ids.scrape_links** takes the arguments:\n",
    "- **method**: Which library to use for scraping, either **bs4** or **selenium**.\n",
    "- **url**: The URL from which to scrape links.\n",
    "- **regex**: Optional. If provided, will filter the scraped links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//oshiete.goo.ne.jp/',\n",
       " 'javascript:void 0;',\n",
       " 'javascript:void 0;',\n",
       " '//oshiete.goo.ne.jp/',\n",
       " '//oshiete.goo.ne.jp/articles/qa/']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fineweb_tools.collect_ids import scrape_links\n",
    "\n",
    "links = scrape_links(\n",
    "    method='bs4',\n",
    "    url=urls[0]\n",
    ")\n",
    "links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use **count_path_branches** to tune a filtering regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/watch', 167),\n",
       " ('/watch/entry', 95),\n",
       " ('/watch/category', 48),\n",
       " ('/articles', 21),\n",
       " ('/articles/qa', 21)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_path_branches(links).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, target content is presented on urls with the branch, **watch/entry/{. . .}**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper does it's job, but most of the links are not useful. Using the argument, **regex**, we can filter the links using regular expressions.\n",
    "\n",
    "Let's try scraping links again using a regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/watch/entry/9efe66583ef412bb0c90d4be18e67d99/?from=entry_side_rank',\n",
       " '/watch/entry/f79dcd164843bd95d8baff8d21e80120/',\n",
       " '/watch/entry/4786777415b37917a79acfa64327b629/']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_links(\n",
    "    method = 'bs4',\n",
    "    url=urls[0],\n",
    "    regex='watch/entry/.+'\n",
    ")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you combine any one of those paths with the base, oshiete.goo.ne.jp, it will take you to an article that is flagged as an expert.\n",
    "\n",
    "URL encoding varies from site to site. It is important that you experiment with your regex to ensure that it is filtering properly.\n",
    "\n",
    "Next, let's try crawling. The function **fineweb_tools.collect_ids.crawl** takes these arguments:\n",
    "- **method**: Scraping library, **bs4** or **selenium**.\n",
    "- **urls**: A list of URLs to scrape.\n",
    "- **output_path**: The path to save your list of scraped links.\n",
    "- **regex**: The regular expression used to filter scraped links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b73f323ed34099a7226d3d5a453cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Crawling and scraping links:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl complete. 779 links scraped\n"
     ]
    }
   ],
   "source": [
    "from fineweb_tools.collect_ids import crawl\n",
    "\n",
    "crawl(\n",
    "    method='bs4',\n",
    "    urls=urls,\n",
    "    output_path='hub_dataset/jpn_Jpan/links/oshiete.txt',\n",
    "    regex='watch/entry/.+'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try scraping using Selenium.\n",
    "\n",
    "## Scraping with Selenium  \n",
    "\n",
    "Selenium is a more flexible library because, unlike BeautifulSoup4, it can interact with web pages as a normal user. Therefore, it can handle things like lazy loading and JavaScript.\n",
    "\n",
    "To demonstrate, we will scrape **Qiita**, which is a developer's forum where users write explanatory articles.\n",
    "\n",
    "In the case of **Qiita**, the article retrieval system is scripted in JavaScript, so BeautifulSoup4 is ineffective.  \n",
    "\n",
    "Although it is more flexible, Selenium is also much slower. Nevertheless, it is quite easy to use.  \n",
    "\n",
    "Just like before, let's start by visiting the **Qiita** index [page](https://qiita.com/search).  \n",
    "\n",
    "In the search bar, click on the icon on the right, which provides a list of search parameters. We are going to use the following ones:  .\n",
    "- **created**: <=2024-04-24 (the date of the most recent article in Japanese FineWeb)  \n",
    "- **sort**: by Likes (we are assuming articles with more Likes are of higher quality)  \n",
    "\n",
    "\n",
    "Plug in the query parameters, and the URL looks like this.\n",
    "\n",
    "`https://qiita.com/search?q=created%3C%3D2024-04-24&sort=like&stocked=&page=1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://qiita.com/search?q=created%3C%3D2024-04-24&sort=like&stocked=&page=1',\n",
       " 'https://qiita.com/search?q=created%3C%3D2024-04-24&sort=like&stocked=&page=2',\n",
       " 'https://qiita.com/search?q=created%3C%3D2024-04-24&sort=like&stocked=&page=3']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = pagerange('https://qiita.com/search?q=created%3C%3D2024-04-24&sort=like&stocked=&page={}', 1, 5)\n",
    "urls[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape links from a single page, and use that information to tune your regex. When using Selenium, it will open up a webbrowser as it scrapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/tags', 86),\n",
       " ('/torifukukaiou', 12),\n",
       " ('/organizations', 11),\n",
       " ('/', 8),\n",
       " ('/shirok', 8),\n",
       " ('/torifukukaiou/items', 6),\n",
       " ('/tags/python', 5),\n",
       " ('/KEINOS', 4),\n",
       " ('/ddd_nnuco', 4),\n",
       " ('/aguilarklyno', 4)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fineweb_tools.collect_ids import scrape_links, count_path_branches\n",
    "\n",
    "links = scrape_links(\n",
    "    method='selenium', #this time, use selenium instead of 'bs4',\n",
    "    url=urls[0]\n",
    ")\n",
    "\n",
    "count_path_branches(links).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern is not as obvious. Content is presented in URLs with the structure:\n",
    "\n",
    "qiita.com/{user}/items/{title}\n",
    "\n",
    "You can target this pattern with this regex:\n",
    "\n",
    "com/.+/items/.+\n",
    "\n",
    "Following **com/**, matches **any sequence of characters**, followed by **/items/**, followed by **any sequence of characters**.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://qiita.com/nucomiya/items/c30c8de57eba7ccdfbe3',\n",
       " 'https://qiita.com/topi_log/items/b8cc6afaa6e12599ffbb',\n",
       " 'https://qiita.com/nukipei/items/f096a1df6c8074b16150',\n",
       " 'https://qiita.com/GOROman/items/769bf17589d5661f7a70',\n",
       " 'https://qiita.com/free-honda/items/8c5c3ec4cdb6ad6a5107']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_links(\n",
    "    method='selenium', #this time, use selenium instead of 'bs4',\n",
    "    url=links[0],\n",
    "    regex = 'com/.+/items/.+'\n",
    ")[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on one of the links to confirm that it directs you to an article page.\n",
    "\n",
    "Finally, let's run a crawl. If you have a lot of pages, this will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a10d1b63e342b1993fc0ce31fe2a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Crawling and scraping links:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl complete. 2106 links scraped\n"
     ]
    }
   ],
   "source": [
    "from fineweb_tools.collect_ids import crawl\n",
    "\n",
    "crawl(\n",
    "    method='selenium',\n",
    "    urls = urls,\n",
    "    output_path = 'hub_dataset/jpn_Jpan/links/qiita.txt',\n",
    "    regex = \".com/.+/items/.+$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have links from a few different pages, let's retrieve IDs from FineWeb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve IDs from FineWeb\n",
    "\n",
    "This step is extremely straightforward. We will use the function, fineweb_tools.collect_ids.id_retrieval_pipeline. This function iterates through FineWeb files, filters rows with matching links, retrieves IDs, and writes them to a text file.\n",
    "\n",
    "The **id_retrieval_pipeline** takes four arguments:\n",
    "- **data_dir**: Path to FineWeb data. I reccommend that you used the **stripped data** from from preprocessing because it is more efficient. The pipeline works fine on raw data too.\n",
    "- **domain**: The domain name from which you scraped links. Used for initial filtering.\n",
    "- **links_path**: Path to the text file where links are saved.\n",
    "- **output_path**: Path to where the list of IDs is to be saved.\n",
    "\n",
    "Let's start with **Mathematica**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b193b5950043cea2e5c964c08a32d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding IDs:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs extracted. 93 found.\n"
     ]
    }
   ],
   "source": [
    "from fineweb_tools.collect_ids import id_retrieval_pipeline\n",
    "\n",
    "id_retrieval_pipeline(\n",
    "    data_dir='FineWeb/intermediate/jpn_Jpan/stripped',\n",
    "    domain='mathematica.site',\n",
    "    links_path='hub_dataset/jpn_Jpan/links/mathematica.txt',\n",
    "    output_path='hub_dataset/jpn_Jpan/ids/mathematica.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do Oshiete and Qiita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06dcccf32e342b08f20421f720c4ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding IDs:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs extracted. 586 found.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0625251cca24196a44842584505e877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding IDs:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_retrieval_pipeline(\n",
    "    data_dir='FineWeb/intermediate/jpn_Jpan/stripped',\n",
    "    domain='qiita',\n",
    "    links_path='hub_dataset/jpn_Jpan/links/qiita.txt',\n",
    "    output_path='hub_dataset/jpn_Jpan/ids/qiita.txt'\n",
    ")\n",
    "\n",
    "id_retrieval_pipeline(\n",
    "    data_dir='FineWeb/intermediate/jpn_Jpan/stripped',\n",
    "    domain='oshiete.goo',\n",
    "    links_path='hub_dataset/jpn_Jpan/links/oshiete.txt',\n",
    "    output_path='hub_dataset/jpn_Jpan/ids/oshiete.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With the list of IDs in hand, we can now prepare a sample for annotation that prioritizes better educational density. While this methodology requires significant effort, it is far less labor-intensive than data annotation itself.\n",
    "\n",
    "Data annotation is a monotonous task, and the attention of our annotators is a valuable resource. By implementing strategies for pre-screening data, we can optimize this process and achieve greater efficiency in the long run.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
